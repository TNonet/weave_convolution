{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Input\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return (0.00100 - 7e-6*epoch)\n",
    "\n",
    "def straight_stack(x, num_filters, rep = 3, weight_decay = 1e-4, dropout_start = 0.2, dropout_add = .1):\n",
    "    for n in range(rep):\n",
    "        x = Conv2D((2**n)*num_filters, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D((2**n)*num_filters, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPool2D()(x)\n",
    "        x = Dropout(dropout_start+n*dropout_add)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(x_train.shape)\n",
    " \n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 16)\n",
      "GPU settings allow for inf layers\n",
      "Minimum output size allow for 6 layers\n",
      "Number of layers 2\n",
      "First Layer Size: 2 (Number of Units)\n",
      "('Disjoint:', True)\n",
      "number of units 2\n",
      "number of devices 4\n",
      "number of inputs 4\n",
      "First Layer Output Size: 2 (Number of Tensors)\n",
      "('Disjoint:', True)\n",
      "number of units 1\n",
      "number of devices 4\n",
      "number of inputs 2\n",
      "Final Layer Size 1\n",
      "('output shape of conv layers: ', TensorShape([Dimension(None), Dimension(4), Dimension(16), Dimension(16)]))\n",
      "('number of parameters %d', 11926)\n",
      "Epoch 1/30\n",
      "781/781 [==============================] - 516s 660ms/step - loss: 1.9272 - acc: 0.2974 - val_loss: 1.7496 - val_acc: 0.3720\n",
      "Epoch 2/30\n",
      "781/781 [==============================] - 527s 675ms/step - loss: 1.7273 - acc: 0.3730 - val_loss: 1.6566 - val_acc: 0.3820\n",
      "Epoch 3/30\n",
      "781/781 [==============================] - 527s 674ms/step - loss: 1.6687 - acc: 0.3967 - val_loss: 1.5621 - val_acc: 0.4480\n",
      "Epoch 4/30\n",
      "781/781 [==============================] - 523s 670ms/step - loss: 1.6291 - acc: 0.4060 - val_loss: 1.5293 - val_acc: 0.4700\n",
      "Epoch 5/30\n",
      "781/781 [==============================] - 524s 671ms/step - loss: 1.5920 - acc: 0.4222 - val_loss: 1.5324 - val_acc: 0.4640\n",
      "Epoch 6/30\n",
      "781/781 [==============================] - 524s 671ms/step - loss: 1.5736 - acc: 0.4298 - val_loss: 1.5173 - val_acc: 0.4460\n",
      "Epoch 7/30\n",
      "781/781 [==============================] - 526s 673ms/step - loss: 1.5473 - acc: 0.4403 - val_loss: 1.4655 - val_acc: 0.4800\n",
      "Epoch 8/30\n",
      "781/781 [==============================] - 526s 673ms/step - loss: 1.5283 - acc: 0.4478 - val_loss: 1.4663 - val_acc: 0.4840\n",
      "Epoch 9/30\n",
      "781/781 [==============================] - 525s 672ms/step - loss: 1.5152 - acc: 0.4535 - val_loss: 1.4674 - val_acc: 0.4700\n",
      "Epoch 10/30\n",
      "781/781 [==============================] - 528s 676ms/step - loss: 1.5009 - acc: 0.4598 - val_loss: 1.4290 - val_acc: 0.4740\n",
      "Epoch 11/30\n",
      "781/781 [==============================] - 528s 676ms/step - loss: 1.4924 - acc: 0.4657 - val_loss: 1.4512 - val_acc: 0.4620\n",
      "Epoch 12/30\n",
      "781/781 [==============================] - 534s 683ms/step - loss: 1.4840 - acc: 0.4650 - val_loss: 1.4230 - val_acc: 0.4760\n",
      "Epoch 13/30\n",
      "781/781 [==============================] - 529s 677ms/step - loss: 1.4764 - acc: 0.4720 - val_loss: 1.4186 - val_acc: 0.4860\n",
      "Epoch 14/30\n",
      "781/781 [==============================] - 530s 678ms/step - loss: 1.4771 - acc: 0.4705 - val_loss: 1.4107 - val_acc: 0.4960\n",
      "Epoch 15/30\n",
      "781/781 [==============================] - 530s 678ms/step - loss: 1.4668 - acc: 0.4734 - val_loss: 1.3884 - val_acc: 0.4900\n",
      "Epoch 16/30\n",
      "781/781 [==============================] - 530s 678ms/step - loss: 1.4633 - acc: 0.4739 - val_loss: 1.4108 - val_acc: 0.4860\n",
      "Epoch 17/30\n",
      "781/781 [==============================] - 533s 683ms/step - loss: 1.4598 - acc: 0.4755 - val_loss: 1.3814 - val_acc: 0.4880\n",
      "Epoch 18/30\n",
      "781/781 [==============================] - 531s 679ms/step - loss: 1.4533 - acc: 0.4792 - val_loss: 1.3864 - val_acc: 0.4920\n",
      "Epoch 19/30\n",
      "781/781 [==============================] - 530s 678ms/step - loss: 1.4476 - acc: 0.4823 - val_loss: 1.4354 - val_acc: 0.4820\n",
      "Epoch 20/30\n",
      "781/781 [==============================] - 530s 679ms/step - loss: 1.4462 - acc: 0.4807 - val_loss: 1.4074 - val_acc: 0.4840\n",
      "Epoch 21/30\n",
      "781/781 [==============================] - 531s 680ms/step - loss: 1.4473 - acc: 0.4816 - val_loss: 1.3675 - val_acc: 0.4860\n",
      "Epoch 22/30\n",
      "781/781 [==============================] - 530s 679ms/step - loss: 1.4370 - acc: 0.4854 - val_loss: 1.4439 - val_acc: 0.4500\n",
      "Epoch 23/30\n",
      "781/781 [==============================] - 530s 678ms/step - loss: 1.4381 - acc: 0.4864 - val_loss: 1.3883 - val_acc: 0.4860\n",
      "Epoch 24/30\n",
      "781/781 [==============================] - 530s 679ms/step - loss: 1.4328 - acc: 0.4874 - val_loss: 1.3902 - val_acc: 0.4800\n",
      "Epoch 25/30\n",
      "781/781 [==============================] - 530s 679ms/step - loss: 1.4326 - acc: 0.4869 - val_loss: 1.3581 - val_acc: 0.4780\n",
      "Epoch 26/30\n",
      "781/781 [==============================] - 531s 680ms/step - loss: 1.4306 - acc: 0.4869 - val_loss: 1.4605 - val_acc: 0.4720\n",
      "Epoch 27/30\n",
      "781/781 [==============================] - 530s 678ms/step - loss: 1.4254 - acc: 0.4882 - val_loss: 1.4472 - val_acc: 0.4740\n",
      "Epoch 28/30\n",
      "781/781 [==============================] - 531s 679ms/step - loss: 1.4272 - acc: 0.4890 - val_loss: 1.4009 - val_acc: 0.4760\n",
      "Epoch 29/30\n",
      "781/781 [==============================] - 531s 680ms/step - loss: 1.4213 - acc: 0.4906 - val_loss: 1.4139 - val_acc: 0.4800\n",
      "Epoch 30/30\n",
      "781/781 [==============================] - 530s 679ms/step - loss: 1.4145 - acc: 0.4938 - val_loss: 1.4119 - val_acc: 0.4760\n",
      "10000/10000 [==============================] - 48s 5ms/step\n",
      "{4: (11926, [1.3469636142730712, 0.5209])}\n",
      "(4, 8, 8)\n",
      "GPU settings allow for inf layers\n",
      "Minimum output size allow for 4 layers\n",
      "Number of layers 1\n",
      "First Layer Size: 1 (Number of Units)\n",
      "('Disjoint:', True)\n",
      "number of units 1\n",
      "number of devices 2\n",
      "number of inputs 2\n",
      "First Layer Output Size: 1 (Number of Tensors)\n",
      "Final Layer Size 1\n",
      "('output shape of conv layers: ', TensorShape([Dimension(None), Dimension(8), Dimension(8), Dimension(8)]))\n",
      "('number of parameters %d', 6894)\n",
      "Epoch 1/30\n",
      "781/781 [==============================] - 437s 560ms/step - loss: 1.8615 - acc: 0.3197 - val_loss: 1.5947 - val_acc: 0.4480\n",
      "Epoch 2/30\n",
      "781/781 [==============================] - 434s 555ms/step - loss: 1.6006 - acc: 0.4164 - val_loss: 1.5284 - val_acc: 0.4400\n",
      "Epoch 3/30\n",
      "781/781 [==============================] - 433s 554ms/step - loss: 1.5308 - acc: 0.4457 - val_loss: 1.4875 - val_acc: 0.4740\n",
      "Epoch 4/30\n",
      "781/781 [==============================] - 434s 556ms/step - loss: 1.4937 - acc: 0.4600 - val_loss: 1.5282 - val_acc: 0.4600\n",
      "Epoch 5/30\n",
      "781/781 [==============================] - 434s 556ms/step - loss: 1.4695 - acc: 0.4702 - val_loss: 1.4782 - val_acc: 0.4640\n",
      "Epoch 6/30\n",
      "781/781 [==============================] - 434s 555ms/step - loss: 1.4464 - acc: 0.4786 - val_loss: 1.4667 - val_acc: 0.4680\n",
      "Epoch 7/30\n",
      "781/781 [==============================] - 434s 556ms/step - loss: 1.4318 - acc: 0.4822 - val_loss: 1.5666 - val_acc: 0.4560\n",
      "Epoch 8/30\n",
      "781/781 [==============================] - 434s 555ms/step - loss: 1.4178 - acc: 0.4907 - val_loss: 1.4604 - val_acc: 0.4800\n",
      "Epoch 9/30\n",
      "781/781 [==============================] - 435s 557ms/step - loss: 1.4069 - acc: 0.4920 - val_loss: 1.4933 - val_acc: 0.4640\n",
      "Epoch 10/30\n",
      "781/781 [==============================] - 435s 556ms/step - loss: 1.3924 - acc: 0.4998 - val_loss: 1.4862 - val_acc: 0.4680\n",
      "Epoch 11/30\n",
      "781/781 [==============================] - 434s 556ms/step - loss: 1.3805 - acc: 0.5063 - val_loss: 1.4726 - val_acc: 0.4800\n",
      "Epoch 12/30\n",
      "781/781 [==============================] - 434s 556ms/step - loss: 1.3802 - acc: 0.5043 - val_loss: 1.4572 - val_acc: 0.4760\n",
      "Epoch 13/30\n",
      "781/781 [==============================] - 434s 556ms/step - loss: 1.3766 - acc: 0.5035 - val_loss: 1.4738 - val_acc: 0.4620\n",
      "Epoch 14/30\n",
      "781/781 [==============================] - 451s 578ms/step - loss: 1.3571 - acc: 0.5111 - val_loss: 1.4741 - val_acc: 0.4680\n",
      "Epoch 15/30\n",
      "781/781 [==============================] - 437s 559ms/step - loss: 1.3591 - acc: 0.5151 - val_loss: 1.4941 - val_acc: 0.4760\n",
      "Epoch 16/30\n",
      "781/781 [==============================] - 434s 556ms/step - loss: 1.3554 - acc: 0.5154 - val_loss: 1.4176 - val_acc: 0.4920\n",
      "Epoch 17/30\n",
      "781/781 [==============================] - 437s 559ms/step - loss: 1.3524 - acc: 0.5161 - val_loss: 1.4409 - val_acc: 0.4720\n",
      "Epoch 18/30\n",
      "781/781 [==============================] - 435s 557ms/step - loss: 1.3460 - acc: 0.5194 - val_loss: 1.4259 - val_acc: 0.4800\n",
      "Epoch 19/30\n",
      "781/781 [==============================] - 435s 557ms/step - loss: 1.3434 - acc: 0.5195 - val_loss: 1.4317 - val_acc: 0.4880\n",
      "Epoch 20/30\n",
      "781/781 [==============================] - 435s 557ms/step - loss: 1.3441 - acc: 0.5169 - val_loss: 1.5266 - val_acc: 0.4580\n",
      "Epoch 21/30\n",
      "781/781 [==============================] - 435s 558ms/step - loss: 1.3381 - acc: 0.5204 - val_loss: 1.5440 - val_acc: 0.4620\n",
      "Epoch 22/30\n",
      "781/781 [==============================] - 435s 557ms/step - loss: 1.3354 - acc: 0.5191 - val_loss: 1.4786 - val_acc: 0.4780\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 436s 558ms/step - loss: 1.3336 - acc: 0.5205 - val_loss: 1.5249 - val_acc: 0.4460\n",
      "Epoch 24/30\n",
      "781/781 [==============================] - 435s 557ms/step - loss: 1.3338 - acc: 0.5221 - val_loss: 1.4937 - val_acc: 0.4700\n",
      "Epoch 25/30\n",
      "781/781 [==============================] - 436s 558ms/step - loss: 1.3295 - acc: 0.5239 - val_loss: 1.3993 - val_acc: 0.4860\n",
      "Epoch 26/30\n",
      "781/781 [==============================] - 435s 557ms/step - loss: 1.3289 - acc: 0.5253 - val_loss: 1.4573 - val_acc: 0.4900\n",
      "Epoch 27/30\n",
      "781/781 [==============================] - 436s 558ms/step - loss: 1.3270 - acc: 0.5263 - val_loss: 1.4558 - val_acc: 0.4840\n",
      "Epoch 28/30\n",
      "781/781 [==============================] - 435s 557ms/step - loss: 1.3186 - acc: 0.5309 - val_loss: 1.4503 - val_acc: 0.4760\n",
      "Epoch 29/30\n",
      "781/781 [==============================] - 436s 558ms/step - loss: 1.3175 - acc: 0.5280 - val_loss: 1.4663 - val_acc: 0.4760\n",
      "Epoch 30/30\n",
      "781/781 [==============================] - 435s 557ms/step - loss: 1.3139 - acc: 0.5310 - val_loss: 1.5546 - val_acc: 0.4820\n",
      "10000/10000 [==============================] - 47s 5ms/step\n",
      "{2: (6894, [1.4662439916610719, 0.5037]), 4: (11926, [1.3469636142730712, 0.5209])}\n",
      "('output shape of conv layers: ', TensorShape([Dimension(None), Dimension(16), Dimension(4), Dimension(4)]))\n",
      "('number of parameters %d', 7646)\n",
      "Epoch 1/30\n",
      "781/781 [==============================] - 418s 535ms/step - loss: 1.9851 - acc: 0.2922 - val_loss: 1.6478 - val_acc: 0.3860\n",
      "Epoch 2/30\n",
      "781/781 [==============================] - 417s 533ms/step - loss: 1.6401 - acc: 0.3973 - val_loss: 1.5462 - val_acc: 0.4300\n",
      "Epoch 3/30\n",
      "781/781 [==============================] - 417s 534ms/step - loss: 1.5459 - acc: 0.4356 - val_loss: 1.4552 - val_acc: 0.4700\n",
      "Epoch 4/30\n",
      "781/781 [==============================] - 417s 534ms/step - loss: 1.4952 - acc: 0.4570 - val_loss: 1.4244 - val_acc: 0.4780\n",
      "Epoch 5/30\n",
      "781/781 [==============================] - 418s 535ms/step - loss: 1.4563 - acc: 0.4724 - val_loss: 1.4445 - val_acc: 0.4700\n",
      "Epoch 6/30\n",
      "781/781 [==============================] - 419s 536ms/step - loss: 1.4171 - acc: 0.4877 - val_loss: 1.3174 - val_acc: 0.5240\n",
      "Epoch 7/30\n",
      "781/781 [==============================] - 417s 534ms/step - loss: 1.4043 - acc: 0.4893 - val_loss: 1.3678 - val_acc: 0.4960\n",
      "Epoch 8/30\n",
      "781/781 [==============================] - 418s 535ms/step - loss: 1.3791 - acc: 0.5008 - val_loss: 1.3469 - val_acc: 0.5060\n",
      "Epoch 9/30\n",
      "781/781 [==============================] - 418s 536ms/step - loss: 1.3652 - acc: 0.5086 - val_loss: 1.3454 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "781/781 [==============================] - 417s 535ms/step - loss: 1.3546 - acc: 0.5116 - val_loss: 1.3099 - val_acc: 0.5100\n",
      "Epoch 11/30\n",
      "781/781 [==============================] - 422s 541ms/step - loss: 1.3505 - acc: 0.5150 - val_loss: 1.3576 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "781/781 [==============================] - 427s 546ms/step - loss: 1.3375 - acc: 0.5212 - val_loss: 1.3099 - val_acc: 0.5060\n",
      "Epoch 13/30\n",
      "781/781 [==============================] - 429s 550ms/step - loss: 1.3240 - acc: 0.5239 - val_loss: 1.2696 - val_acc: 0.5320\n",
      "Epoch 14/30\n",
      "781/781 [==============================] - 421s 538ms/step - loss: 1.3257 - acc: 0.5254 - val_loss: 1.3455 - val_acc: 0.5040\n",
      "Epoch 15/30\n",
      "781/781 [==============================] - 418s 535ms/step - loss: 1.3145 - acc: 0.5311 - val_loss: 1.2483 - val_acc: 0.5460\n",
      "Epoch 16/30\n",
      "781/781 [==============================] - 418s 535ms/step - loss: 1.3025 - acc: 0.5366 - val_loss: 1.3028 - val_acc: 0.5220\n",
      "Epoch 17/30\n",
      "781/781 [==============================] - 418s 536ms/step - loss: 1.3041 - acc: 0.5336 - val_loss: 1.2775 - val_acc: 0.5360\n",
      "Epoch 18/30\n",
      "781/781 [==============================] - 418s 536ms/step - loss: 1.2952 - acc: 0.5392 - val_loss: 1.2847 - val_acc: 0.5400\n",
      "Epoch 19/30\n",
      "781/781 [==============================] - 419s 536ms/step - loss: 1.2839 - acc: 0.5428 - val_loss: 1.2704 - val_acc: 0.5360\n",
      "Epoch 20/30\n",
      "781/781 [==============================] - 419s 537ms/step - loss: 1.2857 - acc: 0.5432 - val_loss: 1.3019 - val_acc: 0.5180\n",
      "Epoch 21/30\n",
      "781/781 [==============================] - 418s 536ms/step - loss: 1.2744 - acc: 0.5479 - val_loss: 1.3035 - val_acc: 0.5140\n",
      "Epoch 22/30\n",
      "781/781 [==============================] - 418s 535ms/step - loss: 1.2747 - acc: 0.5458 - val_loss: 1.2742 - val_acc: 0.5440\n",
      "Epoch 23/30\n",
      "781/781 [==============================] - 419s 536ms/step - loss: 1.2689 - acc: 0.5481 - val_loss: 1.2801 - val_acc: 0.5200\n",
      "Epoch 24/30\n",
      "781/781 [==============================] - 418s 535ms/step - loss: 1.2668 - acc: 0.5491 - val_loss: 1.2301 - val_acc: 0.5480\n",
      "Epoch 25/30\n",
      "781/781 [==============================] - 418s 535ms/step - loss: 1.2640 - acc: 0.5519 - val_loss: 1.2116 - val_acc: 0.5380\n",
      "Epoch 26/30\n",
      "781/781 [==============================] - 418s 535ms/step - loss: 1.2530 - acc: 0.5543 - val_loss: 1.2964 - val_acc: 0.5160\n",
      "Epoch 27/30\n",
      "781/781 [==============================] - 417s 534ms/step - loss: 1.2451 - acc: 0.5595 - val_loss: 1.1767 - val_acc: 0.5500\n",
      "Epoch 28/30\n",
      "781/781 [==============================] - 417s 535ms/step - loss: 1.2456 - acc: 0.5600 - val_loss: 1.3093 - val_acc: 0.5140\n",
      "Epoch 29/30\n",
      "781/781 [==============================] - 418s 536ms/step - loss: 1.2492 - acc: 0.5577 - val_loss: 1.2012 - val_acc: 0.5600\n",
      "Epoch 30/30\n",
      "781/781 [==============================] - 422s 540ms/step - loss: 1.2437 - acc: 0.5581 - val_loss: 1.2346 - val_acc: 0.5540\n",
      "10000/10000 [==============================] - 51s 5ms/step\n",
      "{1: (7646, [1.2174990461349486, 0.5706]), 2: (6894, [1.4662439916610719, 0.5037]), 4: (11926, [1.3469636142730712, 0.5209])}\n"
     ]
    }
   ],
   "source": [
    "viewer = keras.callbacks.TensorBoard(log_dir='./logs', \n",
    "                                     histogram_freq=0, \n",
    "                                     batch_size=batch_size, \n",
    "                                     write_graph=True)\n",
    "num_filters = 4\n",
    "num_val_test = 500\n",
    "epochs = 30\n",
    "model_details = {}\n",
    "conv_layers = 3\n",
    "for num_stack in [4,2,1]:\n",
    "    inputs = Input(shape=(3,32,32))\n",
    "    input_array = []\n",
    "    pyrm_layers = int(np.log2(num_stack))\n",
    "    for stack in range(num_stack):\n",
    "        x = straight_stack(inputs, num_filters/num_stack, rep = conv_layers - pyrm_layers)\n",
    "        input_array.append(x)\n",
    "    if pyrm_layers > 0:\n",
    "        input_size = tuple(input_array[0].shape[1:].as_list())\n",
    "        print(input_size)\n",
    "        x = weave.pyrm_net(input_size=input_size,\n",
    "                           n_layers = pyrm_layers,\n",
    "                           n_filters_start = input_size[0]*2,\n",
    "                           n_gpus = 1,\n",
    "                           inputs = input_array,\n",
    "                           r_filter = 2,\n",
    "                           r_combine= 1,\n",
    "                           min_dim = 2,\n",
    "                           max_pool_loc = 2,\n",
    "                           pure_combine = True,\n",
    "                           end_max_pool = False)\n",
    "    else:\n",
    "        pass\n",
    "    print(\"output shape of conv layers: \",x.shape)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=[inputs], outputs=predictions)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    num_params = model.count_params()\n",
    "    print('number of parameters %d', num_params)\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test[0:num_val_test],y_test[0:num_val_test]),\n",
    "                    callbacks=[LearningRateScheduler(lr_schedule), viewer])\n",
    "    model_details[num_stack] = (num_params, model.evaluate(x_test,y_test))\n",
    "    print(model_details)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (7646, [1.2174990461349486, 0.5706]),\n",
       " 2: (6894, [1.4662439916610719, 0.5037]),\n",
       " 4: (11926, [1.3469636142730712, 0.5209])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 16, 16)\n",
      "GPU settings allow for inf layers\n",
      "Minimum output size allow for 3 layers\n",
      "Number of layers 2\n",
      "First Layer Size: 2 (Number of Units)\n",
      "('Disjoint:', True)\n",
      "number of units 2\n",
      "number of devices 4\n",
      "number of inputs 4\n",
      "First Layer Output Size: 2 (Number of Tensors)\n",
      "('Disjoint:', True)\n",
      "number of units 1\n",
      "number of devices 4\n",
      "number of inputs 2\n",
      "Final Layer Size 1\n",
      "('output shape of conv layers: ', TensorShape([Dimension(None), Dimension(32), Dimension(8), Dimension(8)]))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 3, 32, 32)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 8, 32, 32)    224         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 8, 32, 32)    224         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 32, 32)    224         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 8, 32, 32)    224         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 8, 32, 32)    128         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 8, 32, 32)    128         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 8, 32, 32)    128         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 8, 32, 32)    128         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 8, 32, 32)    584         batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 8, 32, 32)    584         batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 8, 32, 32)    584         batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 8, 32, 32)    584         batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 8, 32, 32)    128         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 8, 32, 32)    128         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 8, 32, 32)    128         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 8, 32, 32)    128         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D) (None, 8, 16, 16)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D) (None, 8, 16, 16)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D) (None, 8, 16, 16)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 8, 16, 16)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 8, 16, 16)    0           max_pooling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 8, 16, 16)    0           max_pooling2d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 8, 16, 16)    0           max_pooling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 8, 16, 16)    0           max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 8, 16, 16)    64          dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 8, 16, 16)    64          dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 8, 16, 16)    64          dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 8, 16, 16)    64          dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 8, 16, 16)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 8, 16, 16)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 8, 16, 16)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 8, 16, 16)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "array_weave_12 (ArrayWeave)     (None, 8, 46, 46)    0           dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_weave_12 (ZeroWeave)       (None, 8, 46, 46)    0           dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "array_weave_11 (ArrayWeave)     (None, 8, 46, 46)    0           dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_weave_11 (ZeroWeave)       (None, 8, 46, 46)    0           dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 46, 46)    0           array_weave_12[0][0]             \n",
      "                                                                 zero_weave_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 46, 46)    0           array_weave_11[0][0]             \n",
      "                                                                 zero_weave_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 8, 48, 48)    0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 8, 48, 48)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 16)   1168        zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 16)   1168        zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D) (None, 16, 8, 8)     0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D) (None, 16, 8, 8)     0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 16, 8, 8)     32          max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 16, 8, 8)     32          max_pooling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 16, 8, 8)     0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 16, 8, 8)     0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "array_weave_13 (ArrayWeave)     (None, 16, 22, 22)   0           dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_weave_13 (ZeroWeave)       (None, 16, 22, 22)   0           dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 22, 22)   0           array_weave_13[0][0]             \n",
      "                                                                 zero_weave_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 16, 24, 24)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 32, 8, 8)     4640        zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 2048)         0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           20490       flatten_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 32,042\n",
      "Trainable params: 31,370\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_filters = 32\n",
    "num_val_test = 500\n",
    "epochs = 65\n",
    "model_details = {}\n",
    "conv_layers = 3\n",
    "num_stack = 4\n",
    "inputs = Input(shape=(3,32,32))\n",
    "input_array = []\n",
    "pyrm_layers = 2\n",
    "for stack in range(num_stack):\n",
    "    x = straight_stack(inputs, num_filters/num_stack, rep = 1)\n",
    "    input_array.append(x)\n",
    "input_size = tuple(input_array[0].shape[1:].as_list())\n",
    "print(input_size)\n",
    "x = weave.pyrm_net(input_size=input_size,\n",
    "                   n_layers = pyrm_layers,\n",
    "                   n_filters_start = input_size[0]*2,\n",
    "                   n_gpus = 1,\n",
    "                   inputs = input_array,\n",
    "                   r_filter = 2,\n",
    "                   r_combine= 1,\n",
    "                   min_dim = 2,\n",
    "                   max_pool_loc = 1,\n",
    "                   pure_combine = True,\n",
    "                   end_max_pool = False)\n",
    "\n",
    "print(\"output shape of conv layers: \",x.shape)\n",
    "x = Flatten()(x)\n",
    "\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=[inputs], outputs=predictions)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n",
      " 66/781 [=>............................] - ETA: 43:11 - loss: 2.0340 - acc: 0.2628"
     ]
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=1,\n",
    "                validation_data=(x_test[0:num_val_test],y_test[0:num_val_test]),\n",
    "                callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
