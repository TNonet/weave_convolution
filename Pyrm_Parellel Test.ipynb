{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tim/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Input\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "import weave\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'channels_first'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return (0.00100 - 7e-6*epoch)\n",
    "\n",
    "def straight_stack(x, num_filters, rep = 3, weight_decay = 1e-4, drop = 0.2):\n",
    "    for n in range(rep):\n",
    "        x = Conv2D((2**n)*num_filters, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D((2**n)*num_filters, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPool2D()(x)\n",
    "        x = Dropout(drop)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(x_train.shape)\n",
    " \n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of filters 4, number of stack 4\n",
      "number of parameters 8202\n",
      "number of filters 4, number of stack 2\n",
      "number of parameters 8150\n",
      "number of filters 4, number of stack 1\n",
      "number of parameters 7646\n",
      "number of filters 8, number of stack 4\n",
      "number of parameters 23874\n",
      "number of filters 8, number of stack 2\n",
      "number of parameters 24530\n",
      "number of filters 8, number of stack 1\n",
      "number of parameters 23762\n",
      "number of filters 16, number of stack 4\n",
      "number of parameters 80922\n",
      "number of filters 16, number of stack 2\n",
      "number of parameters 84506\n",
      "number of filters 16, number of stack 1\n",
      "number of parameters 82778\n",
      "number of filters 32, number of stack 4\n",
      "number of parameters 297834\n",
      "number of filters 32, number of stack 2\n",
      "number of parameters 313322\n",
      "number of filters 32, number of stack 1\n",
      "number of parameters 307946\n"
     ]
    }
   ],
   "source": [
    "# viewer = keras.callbacks.TensorBoard(log_dir='./logs', \n",
    "#                                      histogram_freq=0, \n",
    "#                                      batch_size=batch_size, \n",
    "#                                      write_graph=True)\n",
    "\n",
    "conv_layers = 3\n",
    "filters = [4,8,16,32]\n",
    "num_val_test = 500\n",
    "num_final_test = 10000\n",
    "epochs = 1\n",
    "steps_per_epoch = 1 #x_train.shape[0] // batch_size\n",
    "model_details = {}\n",
    "drop = 0.2\n",
    "for num_filters in filters:\n",
    "    for num_stack in [4,2,1]:\n",
    "        time_callback = TimeHistory()\n",
    "        inputs = Input(shape=(3,32,32))\n",
    "        input_array = []\n",
    "        pyrm_layers = int(np.log2(num_stack))\n",
    "        for stack in range(num_stack):\n",
    "            x = straight_stack(inputs, num_filters/num_stack, rep = conv_layers - pyrm_layers, drop = drop)\n",
    "            input_array.append(x)\n",
    "        if pyrm_layers > 0:\n",
    "            input_size = tuple(input_array[0].shape[1:].as_list())\n",
    "            x = weave.pyrm_net(inputs = input_array,\n",
    "                               n_layers = pyrm_layers,\n",
    "                               n_filters_start = input_size[0]*2,\n",
    "                               n_gpus = 1,\n",
    "                               r_filter = 4,\n",
    "                               r_combine= 2,\n",
    "                               min_dim = 2,\n",
    "                               max_pool_loc = 1,\n",
    "                               pure_combine = False)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Conv2D(x.shape.as_list()[1], (3,3), padding='same', activation='relu')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = MaxPool2D()(x)\n",
    "            x = Dropout(drop)(x)\n",
    "        else:\n",
    "            pass\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        predictions = Dense(num_classes, activation='softmax')(x)\n",
    "        model = Model(inputs=[inputs], outputs=predictions)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        num_params = model.count_params()\n",
    "        print('number of filters %d, number of stack %d' % (num_filters, num_stack))\n",
    "        print('number of parameters %d' % num_params)\n",
    "        hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    steps_per_epoch= steps_per_epoch,\n",
    "                                    epochs=epochs,\n",
    "                                    verbose=0,\n",
    "                                    validation_data=(x_test[0:num_val_test],y_test[0:num_val_test]),\n",
    "                                    callbacks=[LearningRateScheduler(lr_schedule), time_callback]).history\n",
    "        hist['num_params'] = num_params\n",
    "        hist['num_filters'] = num_filters\n",
    "        hist['final accuracy'] = model.evaluate(x_test[0:num_final_test],y_test[0:num_final_test], verbose= 0)\n",
    "        hist['model'] = model.to_json()\n",
    "        hist['times'] = time_callback.times\n",
    "        filepath = str(num_filters) + '_' + str(num_stack) + '_' + str(epochs)\n",
    "        with open(filepath+'.json', 'w') as outfile:\n",
    "            json.dump(hist, outfile)\n",
    "        model.save(filepath+'.h5')\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
