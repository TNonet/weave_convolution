{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead.\n",
    "\n",
    "First you will implement several layer types that are used in convolutional networks. You will then use these layers to train a convolutional network on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cnn import *\n",
    "from data_utils import get_CIFAR10_data\n",
    "from gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from layers import *\n",
    "from fast_layers import *\n",
    "from solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution: Naive backward pass\n",
    "Implement the backward pass for the convolution operation in the function `conv_backward_naive` in the file `layers.py`. Again, you don't need to worry too much about computational efficiency.\n",
    "\n",
    "When you are done, run the following to check your backward pass with a numeric gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "dx zero weave error:  3.27564369120376e-12\n",
      "dx per weave error:  1.0\n",
      "[[ 1.65899061  1.12703316 -0.22099148  0.97284734  1.90966488  0.61153041]\n",
      " [ 0.70363148 -2.48657792  2.28929002 -1.30518567  2.01925756 -1.02794451]\n",
      " [-1.37897476 -1.78990216 -1.4585457  -0.91548224 -0.94248339  0.43612413]\n",
      " [ 0.35856195 -2.30424596 -0.53061762 -0.84938286  0.35725933  0.64950313]\n",
      " [-1.39433831  0.20272528  0.10236235  1.69208288 -1.1298912  -0.82670011]\n",
      " [-1.62233906  0.34280406  0.77613454  1.69955614  0.4740891   0.06848943]]\n",
      "()\n",
      "[[ 0.86458575  0.14169158  0.17411754  0.36579656  1.1478331   0.43392622]\n",
      " [ 1.17443437 -1.37561163  0.80993632 -0.47162512  0.63502818  1.01822012]\n",
      " [-0.13723467 -0.74501265  0.25105608  0.2925804   0.36297842 -0.06198721]\n",
      " [-0.40252646  0.24490312  0.016895   -0.28440095 -0.2555128   0.34267269]\n",
      " [ 0.59994875  0.56103719 -0.74082332 -0.44528355 -0.95009584 -0.50579071]\n",
      " [-1.63133083  0.4219968   0.49883229  0.25691343 -0.71004389  1.71207629]]\n"
     ]
    }
   ],
   "source": [
    "from fancy_conv import *\n",
    "x = np.random.randn(4, 3, 6, 6)\n",
    "y = np.random.randn(4, 3, 6, 6)\n",
    "dout = np.random.randn(4,3,16,16)\n",
    "weave_param = {'num_zeros': 2, 'filter_size': 3}\n",
    "\n",
    "dx_num_zero_weave  = eval_numerical_gradient_array(lambda x: zero_weave_forward(x, weave_param)[0], x, dout)\n",
    "dx_num_per_weave = eval_numerical_gradient_array(lambda x: array_weave_forwards(x, weave_param)[0], x, dout)\n",
    "\n",
    "add_out = array_sum_backwards(x, y)\n",
    "zero_out, zero_cache = zero_weave_forward(x, weave_param)\n",
    "weave_out, weave_cache = array_weave_forwards(x, weave_param)\n",
    "\n",
    "dx_zero = zero_weave_backwards(dout, zero_cache)\n",
    "dx_weave = array_weave_backwards(dout, weave_cache)\n",
    "# Your errors should be around 1e-9'\n",
    "print 'Testing conv_backward_naive function'\n",
    "print 'dx zero weave error: ', rel_error(dx_zero, dx_num_zero_weave)\n",
    "print 'dx per weave error: ', rel_error(dx_num_per_weave, dx_weave)\n",
    "print(dx_num_per_weave[0][0])\n",
    "print()\n",
    "print(dx_weave[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-layer ConvNet\n",
    "Now that you have implemented all the necessary layers, we can put them together into a simple convolutional network.\n",
    "\n",
    "Open the file `cnn.py` and complete the implementation of the `ThreeLayerConvNet` class. Run the following cells to help you debug:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check loss\n",
    "After you build a new network, one of the first things you should do is sanity check the loss. When we use the softmax loss, we expect the loss for random weights (and no regularization) to be about `log(C)` for `C` classes. When we add regularization this should go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss (no regularization):  2.302661410359583\n",
      "Initial loss (with regularization):  2.5102407995740985\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerFancyNet()\n",
    "\n",
    "m = 50\n",
    "X = np.random.randn(m, 3, 32, 32)\n",
    "y = np.random.randint(10, size=m)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print 'Initial loss (no regularization): ', loss\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print 'Initial loss (with regularization): ', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "After the loss looks reasonable, use numeric gradient checking to make sure that your backward pass is correct. When you use numeric gradient checking you should use a small amount of artifical data and a small number of neurons at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_affine_1 max relative error: 3.278778e-02\n",
      "theta_affine_1_0 max relative error: 1.000000e+00\n",
      "theta_affine_2 max relative error: 6.588424e-03\n",
      "theta_affine_2_0 max relative error: 1.134337e-09\n",
      "theta_large max relative error: 2.166741e-02\n",
      "theta_large_0 max relative error: 1.746001e-01\n",
      "theta_loc max relative error: 3.127068e-02\n",
      "theta_loc_0 max relative error: 1.160844e-02\n",
      "theta_per max relative error: 1.000000e+00\n",
      "theta_per_0 max relative error: 4.531490e-01\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerFancyNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit small data\n",
    "A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy (very close to 1.00) and comparatively low validation accuracy (in the 0.20-0.25 range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 400) loss: 2.307388\n",
      "(Epoch 0 / 20) train acc: 0.162000; val_acc: 0.186000\n",
      "(Iteration 11 / 400) loss: 2.146093\n",
      "(Epoch 1 / 20) train acc: 0.322000; val_acc: 0.289000\n",
      "(Iteration 21 / 400) loss: 1.881043\n",
      "(Iteration 31 / 400) loss: 1.914563\n",
      "(Epoch 2 / 20) train acc: 0.409000; val_acc: 0.356000\n",
      "(Iteration 41 / 400) loss: 1.736787\n",
      "(Iteration 51 / 400) loss: 1.464442\n",
      "(Epoch 3 / 20) train acc: 0.491000; val_acc: 0.389000\n",
      "(Iteration 61 / 400) loss: 1.589022\n",
      "(Iteration 71 / 400) loss: 1.297013\n",
      "(Epoch 4 / 20) train acc: 0.566000; val_acc: 0.365000\n",
      "(Iteration 81 / 400) loss: 1.125759\n",
      "(Iteration 91 / 400) loss: 1.048256\n",
      "(Epoch 5 / 20) train acc: 0.654000; val_acc: 0.384000\n",
      "(Iteration 101 / 400) loss: 0.942782\n",
      "(Iteration 111 / 400) loss: 0.924817\n",
      "(Epoch 6 / 20) train acc: 0.748000; val_acc: 0.395000\n",
      "(Iteration 121 / 400) loss: 0.765545\n",
      "(Iteration 131 / 400) loss: 0.616802\n",
      "(Epoch 7 / 20) train acc: 0.846000; val_acc: 0.365000\n",
      "(Iteration 141 / 400) loss: 0.480597\n",
      "(Iteration 151 / 400) loss: 0.593260\n",
      "(Epoch 8 / 20) train acc: 0.881000; val_acc: 0.373000\n",
      "(Iteration 161 / 400) loss: 0.242981\n",
      "(Iteration 171 / 400) loss: 0.348380\n",
      "(Epoch 9 / 20) train acc: 0.913000; val_acc: 0.385000\n",
      "(Iteration 181 / 400) loss: 0.275303\n",
      "(Iteration 191 / 400) loss: 0.174290\n",
      "(Epoch 10 / 20) train acc: 0.956000; val_acc: 0.374000\n",
      "(Iteration 201 / 400) loss: 0.160452\n",
      "(Iteration 211 / 400) loss: 0.099516\n",
      "(Epoch 11 / 20) train acc: 0.970000; val_acc: 0.375000\n",
      "(Iteration 221 / 400) loss: 0.049239\n",
      "(Iteration 231 / 400) loss: 0.053253\n",
      "(Epoch 12 / 20) train acc: 0.973000; val_acc: 0.382000\n",
      "(Iteration 241 / 400) loss: 0.213706\n",
      "(Iteration 251 / 400) loss: 0.068512\n",
      "(Epoch 13 / 20) train acc: 0.990000; val_acc: 0.381000\n",
      "(Iteration 261 / 400) loss: 0.043977\n",
      "(Iteration 271 / 400) loss: 0.014395\n",
      "(Epoch 14 / 20) train acc: 0.995000; val_acc: 0.398000\n",
      "(Iteration 281 / 400) loss: 0.012072\n",
      "(Iteration 291 / 400) loss: 0.008843\n",
      "(Epoch 15 / 20) train acc: 0.992000; val_acc: 0.420000\n",
      "(Iteration 301 / 400) loss: 0.011785\n",
      "(Iteration 311 / 400) loss: 0.008445\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.393000\n",
      "(Iteration 321 / 400) loss: 0.006477\n",
      "(Iteration 331 / 400) loss: 0.001397\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.379000\n",
      "(Iteration 341 / 400) loss: 0.004198\n",
      "(Iteration 351 / 400) loss: 0.001978\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.407000\n",
      "(Iteration 361 / 400) loss: 0.013463\n",
      "(Iteration 371 / 400) loss: 0.000849\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.401000\n",
      "(Iteration 381 / 400) loss: 0.000913\n",
      "(Iteration 391 / 400) loss: 0.001519\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.402000\n"
     ]
    }
   ],
   "source": [
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "model_fanc = ThreeLayerFancyNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model_fanc, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=10)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 400) loss: 0.510336\n",
      "(Epoch 0 / 20) train acc: 0.680000; val_acc: 0.351000\n",
      "(Iteration 11 / 400) loss: 0.748508\n",
      "(Epoch 1 / 20) train acc: 0.805000; val_acc: 0.414000\n",
      "(Iteration 21 / 400) loss: 1.126448\n",
      "(Iteration 31 / 400) loss: 0.500039\n",
      "(Epoch 2 / 20) train acc: 0.874000; val_acc: 0.408000\n",
      "(Iteration 41 / 400) loss: 0.656668\n",
      "(Iteration 51 / 400) loss: 0.277362\n",
      "(Epoch 3 / 20) train acc: 0.885000; val_acc: 0.385000\n",
      "(Iteration 61 / 400) loss: 0.415932\n",
      "(Iteration 71 / 400) loss: 0.377801\n",
      "(Epoch 4 / 20) train acc: 0.932000; val_acc: 0.398000\n",
      "(Iteration 81 / 400) loss: 0.121418\n",
      "(Iteration 91 / 400) loss: 0.394707\n",
      "(Epoch 5 / 20) train acc: 0.938000; val_acc: 0.403000\n",
      "(Iteration 101 / 400) loss: 0.241739\n",
      "(Iteration 111 / 400) loss: 0.219404\n",
      "(Epoch 6 / 20) train acc: 0.953000; val_acc: 0.389000\n",
      "(Iteration 121 / 400) loss: 0.194828\n",
      "(Iteration 131 / 400) loss: 0.141148\n",
      "(Epoch 7 / 20) train acc: 0.971000; val_acc: 0.406000\n",
      "(Iteration 141 / 400) loss: 0.058354\n",
      "(Iteration 151 / 400) loss: 0.037941\n",
      "(Epoch 8 / 20) train acc: 0.988000; val_acc: 0.410000\n",
      "(Iteration 161 / 400) loss: 0.055602\n",
      "(Iteration 171 / 400) loss: 0.053853\n",
      "(Epoch 9 / 20) train acc: 0.985000; val_acc: 0.402000\n",
      "(Iteration 181 / 400) loss: 0.025568\n",
      "(Iteration 191 / 400) loss: 0.015432\n",
      "(Epoch 10 / 20) train acc: 0.995000; val_acc: 0.401000\n",
      "(Iteration 201 / 400) loss: 0.043365\n",
      "(Iteration 211 / 400) loss: 0.008459\n",
      "(Epoch 11 / 20) train acc: 0.993000; val_acc: 0.401000\n",
      "(Iteration 221 / 400) loss: 0.028336\n",
      "(Iteration 231 / 400) loss: 0.013222\n",
      "(Epoch 12 / 20) train acc: 1.000000; val_acc: 0.408000\n",
      "(Iteration 241 / 400) loss: 0.014878\n",
      "(Iteration 251 / 400) loss: 0.010241\n",
      "(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.408000\n",
      "(Iteration 261 / 400) loss: 0.002728\n",
      "(Iteration 271 / 400) loss: 0.003981\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.421000\n",
      "(Iteration 281 / 400) loss: 0.002016\n",
      "(Iteration 291 / 400) loss: 0.001959\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.412000\n",
      "(Iteration 301 / 400) loss: 0.001755\n",
      "(Iteration 311 / 400) loss: 0.002516\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.413000\n",
      "(Iteration 321 / 400) loss: 0.002251\n",
      "(Iteration 331 / 400) loss: 0.001810\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.415000\n",
      "(Iteration 341 / 400) loss: 0.002176\n",
      "(Iteration 351 / 400) loss: 0.002165\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.415000\n",
      "(Iteration 361 / 400) loss: 0.000801\n",
      "(Iteration 371 / 400) loss: 0.002310\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.411000\n",
      "(Iteration 381 / 400) loss: 0.001932\n",
      "(Iteration 391 / 400) loss: 0.001321\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.411000\n"
     ]
    }
   ],
   "source": [
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "model_norm = ThreeLayerNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=10)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c659a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the three-layer convolutional network for one epoch, you should achieve greater than 40% accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 19600) loss: 2.304694\n",
      "(Epoch 0 / 20) train acc: 0.110000; val_acc: 0.105000\n",
      "(Iteration 21 / 19600) loss: 2.258760\n",
      "(Iteration 41 / 19600) loss: 2.164145\n",
      "(Iteration 61 / 19600) loss: 1.950672\n",
      "(Iteration 81 / 19600) loss: 2.239709\n",
      "(Iteration 101 / 19600) loss: 1.837248\n",
      "(Iteration 121 / 19600) loss: 1.875105\n",
      "(Iteration 141 / 19600) loss: 1.599821\n",
      "(Iteration 161 / 19600) loss: 1.715065\n",
      "(Iteration 181 / 19600) loss: 1.687525\n",
      "(Iteration 201 / 19600) loss: 1.645962\n",
      "(Iteration 221 / 19600) loss: 1.826078\n",
      "(Iteration 241 / 19600) loss: 1.685568\n",
      "(Iteration 261 / 19600) loss: 1.700723\n",
      "(Iteration 281 / 19600) loss: 1.759411\n",
      "(Iteration 301 / 19600) loss: 1.922218\n",
      "(Iteration 321 / 19600) loss: 1.286605\n",
      "(Iteration 341 / 19600) loss: 1.586132\n",
      "(Iteration 361 / 19600) loss: 1.645766\n",
      "(Iteration 381 / 19600) loss: 1.378046\n",
      "(Iteration 401 / 19600) loss: 1.384243\n",
      "(Iteration 421 / 19600) loss: 1.548324\n",
      "(Iteration 441 / 19600) loss: 1.609582\n",
      "(Iteration 461 / 19600) loss: 1.671746\n",
      "(Iteration 481 / 19600) loss: 1.783861\n",
      "(Iteration 501 / 19600) loss: 1.425333\n",
      "(Iteration 521 / 19600) loss: 1.579130\n",
      "(Iteration 541 / 19600) loss: 1.277816\n",
      "(Iteration 561 / 19600) loss: 1.220498\n",
      "(Iteration 581 / 19600) loss: 1.527716\n",
      "(Iteration 601 / 19600) loss: 1.627937\n",
      "(Iteration 621 / 19600) loss: 1.484579\n",
      "(Iteration 641 / 19600) loss: 1.121095\n",
      "(Iteration 661 / 19600) loss: 1.585124\n",
      "(Iteration 681 / 19600) loss: 1.537317\n",
      "(Iteration 701 / 19600) loss: 1.331324\n",
      "(Iteration 721 / 19600) loss: 1.521354\n",
      "(Iteration 741 / 19600) loss: 1.088483\n",
      "(Iteration 761 / 19600) loss: 1.577095\n",
      "(Iteration 781 / 19600) loss: 1.326570\n",
      "(Iteration 801 / 19600) loss: 1.269035\n",
      "(Iteration 821 / 19600) loss: 1.441810\n",
      "(Iteration 841 / 19600) loss: 1.024312\n",
      "(Iteration 861 / 19600) loss: 1.376196\n",
      "(Iteration 881 / 19600) loss: 1.513483\n",
      "(Iteration 901 / 19600) loss: 1.549680\n",
      "(Iteration 921 / 19600) loss: 1.509984\n",
      "(Iteration 941 / 19600) loss: 1.527203\n",
      "(Iteration 961 / 19600) loss: 1.481670\n",
      "(Epoch 1 / 20) train acc: 0.575000; val_acc: 0.565000\n",
      "(Iteration 981 / 19600) loss: 1.464570\n",
      "(Iteration 1001 / 19600) loss: 1.495101\n",
      "(Iteration 1021 / 19600) loss: 1.367864\n",
      "(Iteration 1041 / 19600) loss: 1.349435\n",
      "(Iteration 1061 / 19600) loss: 1.093447\n",
      "(Iteration 1081 / 19600) loss: 1.096651\n",
      "(Iteration 1101 / 19600) loss: 1.244827\n",
      "(Iteration 1121 / 19600) loss: 1.568283\n",
      "(Iteration 1141 / 19600) loss: 1.321441\n",
      "(Iteration 1161 / 19600) loss: 1.270167\n",
      "(Iteration 1181 / 19600) loss: 1.294998\n",
      "(Iteration 1201 / 19600) loss: 1.332885\n",
      "(Iteration 1221 / 19600) loss: 1.612267\n",
      "(Iteration 1241 / 19600) loss: 1.357617\n",
      "(Iteration 1261 / 19600) loss: 1.240260\n",
      "(Iteration 1281 / 19600) loss: 1.409798\n",
      "(Iteration 1301 / 19600) loss: 1.332698\n",
      "(Iteration 1321 / 19600) loss: 1.284709\n",
      "(Iteration 1341 / 19600) loss: 1.129863\n",
      "(Iteration 1361 / 19600) loss: 1.513777\n",
      "(Iteration 1381 / 19600) loss: 1.464274\n",
      "(Iteration 1401 / 19600) loss: 1.649797\n",
      "(Iteration 1421 / 19600) loss: 1.336859\n",
      "(Iteration 1441 / 19600) loss: 1.162651\n",
      "(Iteration 1461 / 19600) loss: 1.409199\n",
      "(Iteration 1481 / 19600) loss: 1.236016\n",
      "(Iteration 1501 / 19600) loss: 1.094671\n",
      "(Iteration 1521 / 19600) loss: 1.510091\n",
      "(Iteration 1541 / 19600) loss: 1.303097\n",
      "(Iteration 1561 / 19600) loss: 1.204669\n",
      "(Iteration 1581 / 19600) loss: 1.463780\n",
      "(Iteration 1601 / 19600) loss: 1.217175\n",
      "(Iteration 1621 / 19600) loss: 1.134188\n",
      "(Iteration 1641 / 19600) loss: 1.277724\n",
      "(Iteration 1661 / 19600) loss: 1.062993\n",
      "(Iteration 1681 / 19600) loss: 1.296641\n",
      "(Iteration 1701 / 19600) loss: 1.352346\n",
      "(Iteration 1721 / 19600) loss: 1.448371\n",
      "(Iteration 1741 / 19600) loss: 1.112487\n",
      "(Iteration 1761 / 19600) loss: 1.233074\n",
      "(Iteration 1781 / 19600) loss: 0.988901\n",
      "(Iteration 1801 / 19600) loss: 1.294261\n",
      "(Iteration 1821 / 19600) loss: 1.317049\n",
      "(Iteration 1841 / 19600) loss: 1.249424\n",
      "(Iteration 1861 / 19600) loss: 1.375124\n",
      "(Iteration 1881 / 19600) loss: 1.305620\n",
      "(Iteration 1901 / 19600) loss: 1.456343\n",
      "(Iteration 1921 / 19600) loss: 1.072825\n",
      "(Iteration 1941 / 19600) loss: 1.279636\n",
      "(Epoch 2 / 20) train acc: 0.654000; val_acc: 0.587000\n",
      "(Iteration 1961 / 19600) loss: 0.947591\n",
      "(Iteration 1981 / 19600) loss: 1.059194\n",
      "(Iteration 2001 / 19600) loss: 1.060258\n",
      "(Iteration 2021 / 19600) loss: 1.416493\n",
      "(Iteration 2041 / 19600) loss: 1.047997\n",
      "(Iteration 2061 / 19600) loss: 1.154953\n",
      "(Iteration 2081 / 19600) loss: 1.188769\n",
      "(Iteration 2101 / 19600) loss: 1.372289\n",
      "(Iteration 2121 / 19600) loss: 1.132012\n",
      "(Iteration 2141 / 19600) loss: 1.113279\n",
      "(Iteration 2161 / 19600) loss: 1.311278\n",
      "(Iteration 2181 / 19600) loss: 1.278499\n",
      "(Iteration 2201 / 19600) loss: 1.263812\n",
      "(Iteration 2221 / 19600) loss: 1.445693\n",
      "(Iteration 2241 / 19600) loss: 1.103353\n",
      "(Iteration 2261 / 19600) loss: 1.093907\n",
      "(Iteration 2281 / 19600) loss: 1.105844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6d258ef2b04e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 },\n\u001b[1;32m      9\u001b[0m                 verbose=True, print_every=20)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/Conv/solver.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Maybe print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/Conv/solver.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# Compute loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/Conv/cnn.pyc\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mX_combine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_combine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_sum_fowards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_loc_large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_per_weave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     X_2, cache_large_conv = conv_relu_pool_forward(X_combine, theta_large, \n\u001b[0;32m--> 128\u001b[0;31m         theta_large_0, large_conv_param, pool_param)\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mX_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_affine_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_relu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_affine_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_affine_1_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_affine_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_affine_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_affine_2_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/Conv/layer_utils.pyc\u001b[0m in \u001b[0;36mconv_relu_pool_forward\u001b[0;34m(x, theta, theta0, conv_param, pool_param)\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;34m-\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mObject\u001b[0m \u001b[0mto\u001b[0m \u001b[0mgive\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m   \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_forward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m   \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_pool_forward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/Conv/fast_layers.pyc\u001b[0m in \u001b[0;36mconv_forward_strides\u001b[0;34m(x, w, b, conv_param)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;31m# Pad the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m   \u001b[0mx_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;31m# Figure out output dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/anaconda2/lib/python2.7/site-packages/numpy/lib/arraypad.pyc\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_after\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbefore_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constant_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m             \u001b[0mnewmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepend_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m             \u001b[0mnewmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_append_const\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_after\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/anaconda2/lib/python2.7/site-packages/numpy/lib/arraypad.pyc\u001b[0m in \u001b[0;36m_prepend_const\u001b[0;34m(arr, pad_amt, val, axis)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         return np.concatenate((np.zeros(padshape, dtype=arr.dtype), arr),\n\u001b[0;32m--> 105\u001b[0;31m                               axis=axis)\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         return np.concatenate(((np.zeros(padshape) + val).astype(arr.dtype),\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_full = ThreeLayerFancyNet(weight_scale=0.001, hidden_dim=500, reg=0.001)\n",
    "\n",
    "solver = Solver(model_full, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filters\n",
    "You can visualize the first-layer convolutional filters from the trained network by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_utils import visualize_grid\n",
    "\n",
    "grid = visualize_grid(model.params['theta1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment!\n",
    "Experiment and try to get the best performance that you can on CIFAR-10 using a ConvNet. Here are some ideas to get you started:\n",
    "\n",
    "### Things you should try:\n",
    "- Filter size: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- Number of filters: Above we used 32 filters. Do more or fewer do better?\n",
    "- Network architecture: The network above has two layers of trainable parameters. Can you do better with a deeper network? You can implement alternative architectures in the file `convnet.py`. Some good architectures to try include:\n",
    "    - [conv-relu-pool]xN - conv - relu - [affine]xM - [softmax or SVM]\n",
    "    - [conv-relu-pool]XN - [affine]XM - [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the course-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at least 65% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training, validation, and test set accuracies for your final trained network. In this notebook you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a really good model on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Description\n",
    "If you implement any additional features for extra credit, clearly describe them here with pointers to any code in this or other files if applicable.\n",
    "\n",
    "We implemented a new Non-Linear Function: Exponential Linear Unit.\n",
    "\n",
    "ELU(X) = x for x>0, = exp(x) -1 for x<= 0\n",
    "\n",
    "This can be seen in the following cells, and be found in layers.py and layers utlis.py\n",
    "\n",
    "All following networks use ELUS.\n",
    "\n",
    "We found that ELU conv net memorizes 50 examples faster than the same network with ReLu.\n",
    "\n",
    "We ran two different modesls BestNet and BestNet2 that have their \"geometries\" in the class doc strings.\n",
    "\n",
    "The Last Network we ran get a Val Accuracy of 73.9%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELU (Eponential Linear Unit Check)\n",
    "After preforming some simple research we found a possibly superior method to Leaky ReLu which was ELU and found the implementation very easy.\n",
    "In the following cells we check our implemntation of ELU which can be found in layer_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_elu_pool\n",
      "dx error:  1.5278856439353933e-07\n",
      "dtheta error:  4.4277686235391647e-10\n",
      "dtheta0 error:  1.001467179037312e-11\n"
     ]
    }
   ],
   "source": [
    "from layer_utils import conv_elu_pool_forward, conv_elu_pool_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 16, 16)\n",
    "theta = np.random.randn(3, 3, 3, 3)\n",
    "theta0 = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "out, cache = conv_elu_pool_forward(x, theta, theta0, conv_param, pool_param)\n",
    "dx, dtheta, dtheta0 = conv_elu_pool_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_elu_pool_forward(x, theta, theta0, conv_param, pool_param)[0], x, dout)\n",
    "dtheta_num = eval_numerical_gradient_array(lambda w: conv_elu_pool_forward(x, theta, theta0, conv_param, pool_param)[0], theta, dout)\n",
    "dtheta0_num = eval_numerical_gradient_array(lambda b: conv_elu_pool_forward(x, theta, theta0, conv_param, pool_param)[0], theta0, dout)\n",
    "print 'Testing conv_elu_pool'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dtheta error: ', rel_error(dtheta_num, dtheta)\n",
    "print 'dtheta0 error: ', rel_error(dtheta0_num, dtheta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if our CONV ELU 3 Layer neural network will learn better than the CONV RELU 3 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvEluNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting ELU Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compared to the same Convolution Net with ReLu which can be seen in a previous cell. This network learns faster significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Custom Convolutional Net to Reach 65% acc\n",
    "Code can be found in cnn.py under BestNet, BestNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = BestNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the Net Learsn and nearly memorizes 1000 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 31, 31)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BestNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a4bc5748494d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBestNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m solver = Solver(model, small_data,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BestNet' is not defined"
     ]
    }
   ],
   "source": [
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = BestNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model without regulizaration preforms above 65% but still is overfitted with the full data set. Running it again with a slight regularization of .01 improves over fitting but not perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BestNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BestNet(weight_scale=1e-2, reg = .01)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = BestNet2(weight_scale=1e-2, reg = .01)\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solver = Solver(model3, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = BestNet2(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model3, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = BestNet2(weight_scale=1e-2, reg = 0.02)\n",
    "\n",
    "solver = Solver(model4, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
