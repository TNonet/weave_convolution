{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead.\n",
    "\n",
    "First you will implement several layer types that are used in convolutional networks. You will then use these layers to train a convolutional network on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cnn import *\n",
    "from data_utils import get_CIFAR10_data\n",
    "from gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from layers import *\n",
    "from fast_layers import *\n",
    "from solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_params(model):\n",
    "    num_params = 0\n",
    "    for _,params in model.params.items():\n",
    "        num_params += params.size\n",
    "    return num_params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution: Naive backward pass\n",
    "Implement the backward pass for the convolution operation in the function `conv_backward_naive` in the file `layers.py`. Again, you don't need to worry too much about computational efficiency.\n",
    "\n",
    "When you are done, run the following to check your backward pass with a numeric gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'array_weave_forwards' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-514d84adbd3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdx_num_zero_weave\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mzero_weave_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweave_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdx_num_per_weave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_weave_forwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweave_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0madd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_sum_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/weave_convolution/gradient_check.pyc\u001b[0m in \u001b[0;36meval_numerical_gradient_array\u001b[0;34m(f, x, df, h)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0moldval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moldval\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moldval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-514d84adbd3a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdx_num_zero_weave\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mzero_weave_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweave_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdx_num_per_weave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_weave_forwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweave_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0madd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_sum_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'array_weave_forwards' is not defined"
     ]
    }
   ],
   "source": [
    "from fancy_conv import *\n",
    "size = 10\n",
    "new_size = 3*size - 2\n",
    "x = np.random.randn(4, 3, size, size)\n",
    "y = np.random.randn(4, 3, new_size, new_size)\n",
    "dout = np.random.randn(4,3,new_size,new_size)\n",
    "weave_param = {'num_zeros': 2, 'filter_size': 3}\n",
    "\n",
    "dx_num_zero_weave  = eval_numerical_gradient_array(lambda x: zero_weave_forward(x, weave_param)[0], x, dout)\n",
    "dx_num_per_weave = eval_numerical_gradient_array(lambda x: array_weave_forwards(x, weave_param)[0], x, dout)\n",
    "\n",
    "add_out = array_sum_backwards(x, y)\n",
    "zero_out, zero_cache = zero_weave_forward(x, weave_param)\n",
    "weave_out, weave_cache = array_weave_forwards(x, weave_param)\n",
    "\n",
    "dx_zero = zero_weave_backwards(dout, zero_cache)\n",
    "dx_weave = array_weave_backwards(dout, weave_cache)\n",
    "dx_weave_naive = array_weave_backwards_naive(dout, weave_cache)\n",
    "# Your errors should be around 1e-9'\n",
    "print 'Testing conv_backward_naive function'\n",
    "print 'dx zero weave error: ', rel_error(dx_zero, dx_num_zero_weave)\n",
    "print 'dx per weave error: ', rel_error(dx_num_per_weave, dx_weave)\n",
    "print((dx_weave-dx_weave_naive).mean())\n",
    "print(dx_num_per_weave[0][0])\n",
    "print()\n",
    "print(dx_weave[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-layer ConvNet\n",
    "Now that you have implemented all the necessary layers, we can put them together into a simple convolutional network.\n",
    "\n",
    "Open the file `cnn.py` and complete the implementation of the `ThreeLayerConvNet` class. Run the following cells to help you debug:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check loss\n",
    "After you build a new network, one of the first things you should do is sanity check the loss. When we use the softmax loss, we expect the loss for random weights (and no regularization) to be about `log(C)` for `C` classes. When we add regularization this should go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 32, 32, 32)\n",
      "4\n",
      "Initial loss (no regularization):  2.3025693828350744\n",
      "(50, 32, 32, 32)\n",
      "4\n",
      "Initial loss (with regularization):  2.5106190079933843\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerFancyNet()\n",
    "\n",
    "m = 50\n",
    "X = np.random.randn(m, 3, 32, 32)\n",
    "y = np.random.randint(10, size=m)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print 'Initial loss (no regularization): ', loss\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print 'Initial loss (with regularization): ', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "After the loss looks reasonable, use numeric gradient checking to make sure that your backward pass is correct. When you use numeric gradient checking you should use a small amount of artifical data and a small number of neurons at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_affine_1 max relative error: 3.643688e-02\n",
      "theta_affine_1_0 max relative error: 1.000000e+00\n",
      "theta_affine_2 max relative error: 3.245088e-02\n",
      "theta_affine_2_0 max relative error: 1.796813e-09\n",
      "theta_large max relative error: 3.141255e-01\n",
      "theta_large_0 max relative error: 3.263362e-01\n",
      "theta_loc max relative error: 1.823032e-01\n",
      "theta_loc_0 max relative error: 1.085665e-02\n",
      "theta_per max relative error: 1.000000e+00\n",
      "theta_per_0 max relative error: 1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerFancyNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit small data\n",
    "A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy (very close to 1.00) and comparatively low validation accuracy (in the 0.20-0.25 range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 2.302842\n",
      "(Epoch 0 / 20) train acc: 0.120000; val_acc: 0.105000\n",
      "(Epoch 1 / 20) train acc: 0.230000; val_acc: 0.154000\n",
      "(Epoch 2 / 20) train acc: 0.240000; val_acc: 0.117000\n",
      "(Epoch 3 / 20) train acc: 0.370000; val_acc: 0.135000\n",
      "(Epoch 4 / 20) train acc: 0.370000; val_acc: 0.175000\n",
      "(Epoch 5 / 20) train acc: 0.450000; val_acc: 0.195000\n",
      "(Iteration 11 / 40) loss: 1.509542\n",
      "(Epoch 6 / 20) train acc: 0.480000; val_acc: 0.175000\n",
      "(Epoch 7 / 20) train acc: 0.660000; val_acc: 0.199000\n",
      "(Epoch 8 / 20) train acc: 0.690000; val_acc: 0.195000\n",
      "(Epoch 9 / 20) train acc: 0.690000; val_acc: 0.183000\n",
      "(Epoch 10 / 20) train acc: 0.780000; val_acc: 0.183000\n",
      "(Iteration 21 / 40) loss: 0.703558\n",
      "(Epoch 11 / 20) train acc: 0.830000; val_acc: 0.201000\n",
      "(Epoch 12 / 20) train acc: 0.860000; val_acc: 0.189000\n",
      "(Epoch 13 / 20) train acc: 0.880000; val_acc: 0.184000\n",
      "(Epoch 14 / 20) train acc: 0.920000; val_acc: 0.197000\n",
      "(Epoch 15 / 20) train acc: 0.960000; val_acc: 0.198000\n",
      "(Iteration 31 / 40) loss: 0.069894\n",
      "(Epoch 16 / 20) train acc: 0.970000; val_acc: 0.226000\n",
      "(Epoch 17 / 20) train acc: 0.990000; val_acc: 0.237000\n",
      "(Epoch 18 / 20) train acc: 0.990000; val_acc: 0.224000\n",
      "(Epoch 19 / 20) train acc: 0.990000; val_acc: 0.213000\n",
      "(Epoch 20 / 20) train acc: 0.980000; val_acc: 0.188000\n"
     ]
    }
   ],
   "source": [
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "model_fanc = ThreeLayerFancyNet(weight_scale=1e-2)\n",
    "\n",
    "solver_fanc = Solver(model_fanc, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=10)\n",
    "solver_fanc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 2000) loss: 2.307102\n",
      "(Epoch 0 / 20) train acc: 0.150000; val_acc: 0.146000\n",
      "(Iteration 11 / 2000) loss: 2.018053\n",
      "(Iteration 21 / 2000) loss: 1.673560\n",
      "(Iteration 31 / 2000) loss: 1.891853\n",
      "(Iteration 41 / 2000) loss: 2.094593\n",
      "(Iteration 51 / 2000) loss: 1.932086\n",
      "(Iteration 61 / 2000) loss: 1.784535\n",
      "(Iteration 71 / 2000) loss: 1.832249\n",
      "(Iteration 81 / 2000) loss: 1.537012\n",
      "(Iteration 91 / 2000) loss: 1.445745\n",
      "(Epoch 1 / 20) train acc: 0.479000; val_acc: 0.404000\n",
      "(Iteration 101 / 2000) loss: 1.300947\n",
      "(Iteration 111 / 2000) loss: 1.494999\n",
      "(Iteration 121 / 2000) loss: 1.330065\n",
      "(Iteration 131 / 2000) loss: 1.456870\n",
      "(Iteration 141 / 2000) loss: 1.559031\n",
      "(Iteration 151 / 2000) loss: 1.241629\n",
      "(Iteration 161 / 2000) loss: 1.281059\n",
      "(Iteration 171 / 2000) loss: 1.429651\n",
      "(Iteration 181 / 2000) loss: 1.561921\n",
      "(Iteration 191 / 2000) loss: 1.272349\n",
      "(Epoch 2 / 20) train acc: 0.526000; val_acc: 0.435000\n",
      "(Iteration 201 / 2000) loss: 1.390059\n",
      "(Iteration 211 / 2000) loss: 1.005334\n",
      "(Iteration 221 / 2000) loss: 1.138204\n",
      "(Iteration 231 / 2000) loss: 0.974747\n",
      "(Iteration 241 / 2000) loss: 1.294420\n",
      "(Iteration 251 / 2000) loss: 1.195369\n",
      "(Iteration 261 / 2000) loss: 1.342960\n",
      "(Iteration 271 / 2000) loss: 1.189627\n",
      "(Iteration 281 / 2000) loss: 1.413582\n",
      "(Iteration 291 / 2000) loss: 1.345305\n",
      "(Epoch 3 / 20) train acc: 0.612000; val_acc: 0.479000\n",
      "(Iteration 301 / 2000) loss: 1.177379\n",
      "(Iteration 311 / 2000) loss: 0.887659\n",
      "(Iteration 321 / 2000) loss: 1.267533\n",
      "(Iteration 331 / 2000) loss: 1.314639\n",
      "(Iteration 341 / 2000) loss: 1.009677\n",
      "(Iteration 351 / 2000) loss: 0.859453\n",
      "(Iteration 361 / 2000) loss: 1.156696\n",
      "(Iteration 371 / 2000) loss: 0.988603\n",
      "(Iteration 381 / 2000) loss: 0.846342\n",
      "(Iteration 391 / 2000) loss: 1.224931\n",
      "(Epoch 4 / 20) train acc: 0.646000; val_acc: 0.455000\n",
      "(Iteration 401 / 2000) loss: 0.962431\n",
      "(Iteration 411 / 2000) loss: 0.812743\n",
      "(Iteration 421 / 2000) loss: 1.066186\n",
      "(Iteration 431 / 2000) loss: 0.880474\n",
      "(Iteration 441 / 2000) loss: 0.942110\n",
      "(Iteration 451 / 2000) loss: 1.171437\n",
      "(Iteration 461 / 2000) loss: 1.285202\n",
      "(Iteration 471 / 2000) loss: 1.092044\n",
      "(Iteration 481 / 2000) loss: 0.780436\n",
      "(Iteration 491 / 2000) loss: 0.872494\n",
      "(Epoch 5 / 20) train acc: 0.676000; val_acc: 0.507000\n",
      "(Iteration 501 / 2000) loss: 1.094662\n",
      "(Iteration 511 / 2000) loss: 0.879505\n",
      "(Iteration 521 / 2000) loss: 0.899808\n",
      "(Iteration 531 / 2000) loss: 1.039939\n",
      "(Iteration 541 / 2000) loss: 1.134600\n",
      "(Iteration 551 / 2000) loss: 0.956618\n",
      "(Iteration 561 / 2000) loss: 0.766157\n",
      "(Iteration 571 / 2000) loss: 1.011987\n",
      "(Iteration 581 / 2000) loss: 0.784083\n",
      "(Iteration 591 / 2000) loss: 1.093617\n",
      "(Epoch 6 / 20) train acc: 0.761000; val_acc: 0.487000\n",
      "(Iteration 601 / 2000) loss: 0.751958\n",
      "(Iteration 611 / 2000) loss: 0.824060\n",
      "(Iteration 621 / 2000) loss: 0.783481\n",
      "(Iteration 631 / 2000) loss: 1.016062\n",
      "(Iteration 641 / 2000) loss: 0.812344\n",
      "(Iteration 651 / 2000) loss: 0.845138\n",
      "(Iteration 661 / 2000) loss: 0.846275\n",
      "(Iteration 671 / 2000) loss: 0.757905\n",
      "(Iteration 681 / 2000) loss: 0.768789\n",
      "(Iteration 691 / 2000) loss: 0.709236\n",
      "(Epoch 7 / 20) train acc: 0.771000; val_acc: 0.500000\n",
      "(Iteration 701 / 2000) loss: 0.641244\n",
      "(Iteration 711 / 2000) loss: 0.522784\n",
      "(Iteration 721 / 2000) loss: 0.562363\n",
      "(Iteration 731 / 2000) loss: 0.573914\n",
      "(Iteration 741 / 2000) loss: 0.712298\n",
      "(Iteration 751 / 2000) loss: 0.537478\n",
      "(Iteration 761 / 2000) loss: 0.635166\n",
      "(Iteration 771 / 2000) loss: 0.664256\n",
      "(Iteration 781 / 2000) loss: 0.389257\n",
      "(Iteration 791 / 2000) loss: 0.356334\n",
      "(Epoch 8 / 20) train acc: 0.753000; val_acc: 0.481000\n",
      "(Iteration 801 / 2000) loss: 0.859782\n",
      "(Iteration 811 / 2000) loss: 0.712371\n",
      "(Iteration 821 / 2000) loss: 0.519273\n",
      "(Iteration 831 / 2000) loss: 0.525981\n",
      "(Iteration 841 / 2000) loss: 0.783821\n",
      "(Iteration 851 / 2000) loss: 0.692345\n",
      "(Iteration 861 / 2000) loss: 0.925634\n",
      "(Iteration 871 / 2000) loss: 0.747564\n",
      "(Iteration 881 / 2000) loss: 0.588675\n",
      "(Iteration 891 / 2000) loss: 0.593928\n",
      "(Epoch 9 / 20) train acc: 0.785000; val_acc: 0.454000\n",
      "(Iteration 901 / 2000) loss: 0.432092\n",
      "(Iteration 911 / 2000) loss: 0.545228\n",
      "(Iteration 921 / 2000) loss: 0.490169\n",
      "(Iteration 931 / 2000) loss: 0.502775\n",
      "(Iteration 941 / 2000) loss: 0.664289\n",
      "(Iteration 951 / 2000) loss: 0.361706\n",
      "(Iteration 961 / 2000) loss: 0.514606\n",
      "(Iteration 971 / 2000) loss: 0.576648\n",
      "(Iteration 981 / 2000) loss: 0.499417\n",
      "(Iteration 991 / 2000) loss: 0.471685\n",
      "(Epoch 10 / 20) train acc: 0.814000; val_acc: 0.490000\n",
      "(Iteration 1001 / 2000) loss: 0.483470\n",
      "(Iteration 1011 / 2000) loss: 0.365848\n",
      "(Iteration 1021 / 2000) loss: 0.444945\n",
      "(Iteration 1031 / 2000) loss: 0.413969\n",
      "(Iteration 1041 / 2000) loss: 0.317878\n",
      "(Iteration 1051 / 2000) loss: 0.353522\n",
      "(Iteration 1061 / 2000) loss: 0.646361\n",
      "(Iteration 1071 / 2000) loss: 0.377523\n",
      "(Iteration 1081 / 2000) loss: 0.397774\n",
      "(Iteration 1091 / 2000) loss: 0.433461\n",
      "(Epoch 11 / 20) train acc: 0.872000; val_acc: 0.497000\n",
      "(Iteration 1101 / 2000) loss: 0.438087\n",
      "(Iteration 1111 / 2000) loss: 0.190126\n",
      "(Iteration 1121 / 2000) loss: 0.490223\n",
      "(Iteration 1131 / 2000) loss: 0.389177\n",
      "(Iteration 1141 / 2000) loss: 0.240766\n",
      "(Iteration 1151 / 2000) loss: 0.492369\n",
      "(Iteration 1161 / 2000) loss: 0.573036\n",
      "(Iteration 1171 / 2000) loss: 0.589593\n",
      "(Iteration 1181 / 2000) loss: 0.665599\n",
      "(Iteration 1191 / 2000) loss: 0.600970\n",
      "(Epoch 12 / 20) train acc: 0.824000; val_acc: 0.500000\n",
      "(Iteration 1201 / 2000) loss: 0.433126\n",
      "(Iteration 1211 / 2000) loss: 0.467323\n",
      "(Iteration 1221 / 2000) loss: 0.369672\n",
      "(Iteration 1231 / 2000) loss: 0.402566\n",
      "(Iteration 1241 / 2000) loss: 0.378008\n",
      "(Iteration 1251 / 2000) loss: 0.617355\n",
      "(Iteration 1261 / 2000) loss: 0.216974\n",
      "(Iteration 1271 / 2000) loss: 0.408652\n",
      "(Iteration 1281 / 2000) loss: 0.322132\n",
      "(Iteration 1291 / 2000) loss: 0.220009\n",
      "(Epoch 13 / 20) train acc: 0.890000; val_acc: 0.485000\n",
      "(Iteration 1301 / 2000) loss: 0.334141\n",
      "(Iteration 1311 / 2000) loss: 0.427585\n",
      "(Iteration 1321 / 2000) loss: 0.239320\n",
      "(Iteration 1331 / 2000) loss: 0.297452\n",
      "(Iteration 1341 / 2000) loss: 0.292528\n",
      "(Iteration 1351 / 2000) loss: 0.405476\n",
      "(Iteration 1361 / 2000) loss: 0.230663\n",
      "(Iteration 1371 / 2000) loss: 0.422469\n",
      "(Iteration 1381 / 2000) loss: 0.393682\n",
      "(Iteration 1391 / 2000) loss: 0.276800\n",
      "(Epoch 14 / 20) train acc: 0.876000; val_acc: 0.472000\n",
      "(Iteration 1401 / 2000) loss: 0.221937\n",
      "(Iteration 1411 / 2000) loss: 0.581758\n",
      "(Iteration 1421 / 2000) loss: 0.343423\n",
      "(Iteration 1431 / 2000) loss: 0.389327\n",
      "(Iteration 1441 / 2000) loss: 0.209094\n",
      "(Iteration 1451 / 2000) loss: 0.516212\n",
      "(Iteration 1461 / 2000) loss: 0.277953\n",
      "(Iteration 1471 / 2000) loss: 0.224476\n",
      "(Iteration 1481 / 2000) loss: 0.167738\n",
      "(Iteration 1491 / 2000) loss: 0.187850\n",
      "(Epoch 15 / 20) train acc: 0.910000; val_acc: 0.501000\n",
      "(Iteration 1501 / 2000) loss: 0.163635\n",
      "(Iteration 1511 / 2000) loss: 0.270696\n",
      "(Iteration 1521 / 2000) loss: 0.272643\n",
      "(Iteration 1531 / 2000) loss: 0.570428\n",
      "(Iteration 1541 / 2000) loss: 0.284440\n",
      "(Iteration 1551 / 2000) loss: 0.186836\n",
      "(Iteration 1561 / 2000) loss: 0.274073\n",
      "(Iteration 1571 / 2000) loss: 0.205032\n",
      "(Iteration 1581 / 2000) loss: 0.371238\n",
      "(Iteration 1591 / 2000) loss: 0.180412\n",
      "(Epoch 16 / 20) train acc: 0.911000; val_acc: 0.498000\n",
      "(Iteration 1601 / 2000) loss: 0.168547\n",
      "(Iteration 1611 / 2000) loss: 0.174465\n",
      "(Iteration 1621 / 2000) loss: 0.219966\n",
      "(Iteration 1631 / 2000) loss: 0.177541\n",
      "(Iteration 1641 / 2000) loss: 0.191267\n",
      "(Iteration 1651 / 2000) loss: 0.447052\n",
      "(Iteration 1661 / 2000) loss: 0.227818\n",
      "(Iteration 1671 / 2000) loss: 0.124667\n",
      "(Iteration 1681 / 2000) loss: 0.162688\n",
      "(Iteration 1691 / 2000) loss: 0.190432\n",
      "(Epoch 17 / 20) train acc: 0.929000; val_acc: 0.500000\n",
      "(Iteration 1701 / 2000) loss: 0.386270\n",
      "(Iteration 1711 / 2000) loss: 0.291949\n",
      "(Iteration 1721 / 2000) loss: 0.278614\n",
      "(Iteration 1731 / 2000) loss: 0.441292\n",
      "(Iteration 1741 / 2000) loss: 0.443213\n",
      "(Iteration 1751 / 2000) loss: 0.178741\n",
      "(Iteration 1761 / 2000) loss: 0.141034\n",
      "(Iteration 1771 / 2000) loss: 0.186476\n",
      "(Iteration 1781 / 2000) loss: 0.498664\n",
      "(Iteration 1791 / 2000) loss: 0.511761\n",
      "(Epoch 18 / 20) train acc: 0.917000; val_acc: 0.499000\n",
      "(Iteration 1801 / 2000) loss: 0.291075\n",
      "(Iteration 1811 / 2000) loss: 0.364657\n",
      "(Iteration 1821 / 2000) loss: 0.453220\n",
      "(Iteration 1831 / 2000) loss: 0.536780\n",
      "(Iteration 1841 / 2000) loss: 0.313922\n",
      "(Iteration 1851 / 2000) loss: 0.156694\n",
      "(Iteration 1861 / 2000) loss: 0.156360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1871 / 2000) loss: 0.082005\n",
      "(Iteration 1881 / 2000) loss: 0.063219\n",
      "(Iteration 1891 / 2000) loss: 0.058786\n",
      "(Epoch 19 / 20) train acc: 0.940000; val_acc: 0.505000\n",
      "(Iteration 1901 / 2000) loss: 0.235800\n",
      "(Iteration 1911 / 2000) loss: 0.185078\n",
      "(Iteration 1921 / 2000) loss: 0.168429\n",
      "(Iteration 1931 / 2000) loss: 0.122564\n",
      "(Iteration 1941 / 2000) loss: 0.265986\n",
      "(Iteration 1951 / 2000) loss: 0.355912\n",
      "(Iteration 1961 / 2000) loss: 0.382182\n",
      "(Iteration 1971 / 2000) loss: 0.085451\n",
      "(Iteration 1981 / 2000) loss: 0.186453\n",
      "(Iteration 1991 / 2000) loss: 0.186655\n",
      "(Epoch 20 / 20) train acc: 0.958000; val_acc: 0.477000\n"
     ]
    }
   ],
   "source": [
    "model_norm = ThreeLayerNet(weight_scale=1e-2)\n",
    "\n",
    "solver_norm = Solver(model_norm, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=10)\n",
    "\n",
    "solver_norm.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x180c25af10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(solver_norm.train_acc_history, '-o')\n",
    "plt.plot(solver_norm.val_acc_history, '-o')\n",
    "plt.plot(solver_fanc.train_acc_history, '-o')\n",
    "plt.plot(solver_fanc.val_acc_history, '-o')\n",
    "plt.legend(['train_norm', 'val_norm', 'train_fanc','val_norm'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831350\n",
      "821206\n"
     ]
    }
   ],
   "source": [
    "print(num_params(model_fanc))\n",
    "print(num_params(model_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the three-layer convolutional network for one epoch, you should achieve greater than 40% accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 19600) loss: 2.303180\n",
      "(Epoch 0 / 20) train acc: 0.113000; val_acc: 0.119000\n",
      "(Iteration 21 / 19600) loss: 2.216956\n",
      "(Iteration 41 / 19600) loss: 2.133912\n",
      "(Iteration 61 / 19600) loss: 1.844150\n",
      "(Iteration 81 / 19600) loss: 1.945671\n",
      "(Iteration 101 / 19600) loss: 2.048850\n",
      "(Iteration 121 / 19600) loss: 1.721992\n",
      "(Iteration 141 / 19600) loss: 1.609308\n",
      "(Iteration 161 / 19600) loss: 1.696066\n",
      "(Iteration 181 / 19600) loss: 1.770352\n",
      "(Iteration 201 / 19600) loss: 1.665263\n",
      "(Iteration 221 / 19600) loss: 1.785802\n",
      "(Iteration 241 / 19600) loss: 1.737522\n",
      "(Iteration 261 / 19600) loss: 1.467268\n",
      "(Iteration 281 / 19600) loss: 1.535193\n",
      "(Iteration 301 / 19600) loss: 1.507390\n",
      "(Iteration 321 / 19600) loss: 1.457376\n",
      "(Iteration 341 / 19600) loss: 1.509361\n",
      "(Iteration 361 / 19600) loss: 1.616029\n",
      "(Iteration 381 / 19600) loss: 1.747134\n",
      "(Iteration 401 / 19600) loss: 1.505014\n",
      "(Iteration 421 / 19600) loss: 1.411675\n",
      "(Iteration 441 / 19600) loss: 1.554520\n",
      "(Iteration 461 / 19600) loss: 1.489520\n",
      "(Iteration 481 / 19600) loss: 1.308267\n",
      "(Iteration 501 / 19600) loss: 1.701323\n",
      "(Iteration 521 / 19600) loss: 1.441845\n",
      "(Iteration 541 / 19600) loss: 1.619058\n",
      "(Iteration 561 / 19600) loss: 1.249142\n",
      "(Iteration 581 / 19600) loss: 1.483784\n",
      "(Iteration 601 / 19600) loss: 1.621970\n",
      "(Iteration 621 / 19600) loss: 1.290955\n",
      "(Iteration 641 / 19600) loss: 1.517040\n",
      "(Iteration 661 / 19600) loss: 1.554068\n",
      "(Iteration 681 / 19600) loss: 1.329455\n",
      "(Iteration 701 / 19600) loss: 1.450924\n",
      "(Iteration 721 / 19600) loss: 1.431181\n",
      "(Iteration 741 / 19600) loss: 1.501238\n",
      "(Iteration 761 / 19600) loss: 1.489163\n",
      "(Iteration 781 / 19600) loss: 1.173102\n",
      "(Iteration 801 / 19600) loss: 1.549957\n",
      "(Iteration 821 / 19600) loss: 1.342570\n",
      "(Iteration 841 / 19600) loss: 1.345723\n",
      "(Iteration 861 / 19600) loss: 1.073230\n",
      "(Iteration 881 / 19600) loss: 1.149382\n",
      "(Iteration 901 / 19600) loss: 1.441880\n",
      "(Iteration 921 / 19600) loss: 1.135113\n",
      "(Iteration 941 / 19600) loss: 1.358472\n",
      "(Iteration 961 / 19600) loss: 1.317826\n",
      "(Epoch 1 / 20) train acc: 0.521000; val_acc: 0.495000\n",
      "(Iteration 981 / 19600) loss: 1.543773\n",
      "(Iteration 1001 / 19600) loss: 1.287515\n",
      "(Iteration 1021 / 19600) loss: 1.218291\n",
      "(Iteration 1041 / 19600) loss: 1.431851\n",
      "(Iteration 1061 / 19600) loss: 1.339308\n",
      "(Iteration 1081 / 19600) loss: 1.629062\n",
      "(Iteration 1101 / 19600) loss: 1.360356\n",
      "(Iteration 1121 / 19600) loss: 1.594628\n",
      "(Iteration 1141 / 19600) loss: 1.624372\n",
      "(Iteration 1161 / 19600) loss: 1.129953\n",
      "(Iteration 1181 / 19600) loss: 1.414204\n",
      "(Iteration 1201 / 19600) loss: 1.324280\n",
      "(Iteration 1221 / 19600) loss: 1.531933\n",
      "(Iteration 1241 / 19600) loss: 1.190622\n",
      "(Iteration 1261 / 19600) loss: 1.350513\n",
      "(Iteration 1281 / 19600) loss: 1.337595\n",
      "(Iteration 1301 / 19600) loss: 1.253797\n",
      "(Iteration 1321 / 19600) loss: 1.161151\n",
      "(Iteration 1341 / 19600) loss: 0.963165\n",
      "(Iteration 1361 / 19600) loss: 1.132161\n",
      "(Iteration 1381 / 19600) loss: 1.205043\n",
      "(Iteration 1401 / 19600) loss: 1.043137\n",
      "(Iteration 1421 / 19600) loss: 1.213521\n",
      "(Iteration 1441 / 19600) loss: 1.180551\n",
      "(Iteration 1461 / 19600) loss: 1.142198\n",
      "(Iteration 1481 / 19600) loss: 1.358102\n",
      "(Iteration 1501 / 19600) loss: 1.283883\n",
      "(Iteration 1521 / 19600) loss: 0.974593\n",
      "(Iteration 1541 / 19600) loss: 1.205292\n",
      "(Iteration 1561 / 19600) loss: 1.764830\n",
      "(Iteration 1581 / 19600) loss: 1.139972\n",
      "(Iteration 1601 / 19600) loss: 1.427406\n",
      "(Iteration 1621 / 19600) loss: 1.219014\n",
      "(Iteration 1641 / 19600) loss: 1.035845\n",
      "(Iteration 1661 / 19600) loss: 1.325102\n",
      "(Iteration 1681 / 19600) loss: 1.290288\n",
      "(Iteration 1701 / 19600) loss: 1.158309\n",
      "(Iteration 1721 / 19600) loss: 1.357490\n",
      "(Iteration 1741 / 19600) loss: 1.098872\n",
      "(Iteration 1761 / 19600) loss: 1.247234\n",
      "(Iteration 1781 / 19600) loss: 1.389787\n",
      "(Iteration 1801 / 19600) loss: 1.189087\n",
      "(Iteration 1821 / 19600) loss: 1.281727\n",
      "(Iteration 1841 / 19600) loss: 1.427968\n",
      "(Iteration 1861 / 19600) loss: 1.266130\n",
      "(Iteration 1881 / 19600) loss: 1.434007\n",
      "(Iteration 1901 / 19600) loss: 1.588943\n",
      "(Iteration 1921 / 19600) loss: 1.254587\n",
      "(Iteration 1941 / 19600) loss: 1.367264\n",
      "(Epoch 2 / 20) train acc: 0.636000; val_acc: 0.550000\n",
      "(Iteration 1961 / 19600) loss: 1.635068\n",
      "(Iteration 1981 / 19600) loss: 1.048449\n",
      "(Iteration 2001 / 19600) loss: 1.023308\n",
      "(Iteration 2021 / 19600) loss: 1.291026\n",
      "(Iteration 2041 / 19600) loss: 1.199374\n",
      "(Iteration 2061 / 19600) loss: 1.211723\n",
      "(Iteration 2081 / 19600) loss: 1.378246\n",
      "(Iteration 2101 / 19600) loss: 1.151353\n",
      "(Iteration 2121 / 19600) loss: 1.130504\n",
      "(Iteration 2141 / 19600) loss: 1.077844\n",
      "(Iteration 2161 / 19600) loss: 1.256475\n",
      "(Iteration 2181 / 19600) loss: 1.574319\n",
      "(Iteration 2201 / 19600) loss: 1.443935\n",
      "(Iteration 2221 / 19600) loss: 1.140781\n",
      "(Iteration 2241 / 19600) loss: 1.074468\n",
      "(Iteration 2261 / 19600) loss: 1.179790\n",
      "(Iteration 2281 / 19600) loss: 1.168409\n",
      "(Iteration 2301 / 19600) loss: 1.006256\n",
      "(Iteration 2321 / 19600) loss: 1.194126\n",
      "(Iteration 2341 / 19600) loss: 0.964805\n",
      "(Iteration 2361 / 19600) loss: 1.322389\n",
      "(Iteration 2381 / 19600) loss: 1.222772\n",
      "(Iteration 2401 / 19600) loss: 0.940304\n",
      "(Iteration 2421 / 19600) loss: 1.179885\n",
      "(Iteration 2441 / 19600) loss: 0.925836\n",
      "(Iteration 2461 / 19600) loss: 1.298368\n",
      "(Iteration 2481 / 19600) loss: 1.000158\n",
      "(Iteration 2501 / 19600) loss: 1.043680\n",
      "(Iteration 2521 / 19600) loss: 1.143838\n",
      "(Iteration 2541 / 19600) loss: 0.951882\n",
      "(Iteration 2561 / 19600) loss: 1.179602\n",
      "(Iteration 2581 / 19600) loss: 1.006580\n",
      "(Iteration 2601 / 19600) loss: 0.977428\n",
      "(Iteration 2621 / 19600) loss: 0.987248\n",
      "(Iteration 2641 / 19600) loss: 0.915438\n",
      "(Iteration 2661 / 19600) loss: 1.594215\n",
      "(Iteration 2681 / 19600) loss: 0.981682\n",
      "(Iteration 2701 / 19600) loss: 0.963436\n",
      "(Iteration 2721 / 19600) loss: 1.363687\n",
      "(Iteration 2741 / 19600) loss: 1.294674\n",
      "(Iteration 2761 / 19600) loss: 0.716275\n",
      "(Iteration 2781 / 19600) loss: 1.228416\n",
      "(Iteration 2801 / 19600) loss: 1.157793\n",
      "(Iteration 2821 / 19600) loss: 0.953103\n",
      "(Iteration 2841 / 19600) loss: 0.928957\n",
      "(Iteration 2861 / 19600) loss: 1.209468\n",
      "(Iteration 2881 / 19600) loss: 1.315647\n",
      "(Iteration 2901 / 19600) loss: 1.190605\n",
      "(Iteration 2921 / 19600) loss: 0.968188\n",
      "(Epoch 3 / 20) train acc: 0.659000; val_acc: 0.563000\n",
      "(Iteration 2941 / 19600) loss: 1.136581\n",
      "(Iteration 2961 / 19600) loss: 0.947775\n",
      "(Iteration 2981 / 19600) loss: 1.109825\n",
      "(Iteration 3001 / 19600) loss: 1.119776\n",
      "(Iteration 3021 / 19600) loss: 1.460399\n",
      "(Iteration 3041 / 19600) loss: 1.008623\n",
      "(Iteration 3061 / 19600) loss: 1.128980\n",
      "(Iteration 3081 / 19600) loss: 1.117054\n",
      "(Iteration 3101 / 19600) loss: 1.263621\n",
      "(Iteration 3121 / 19600) loss: 1.374539\n",
      "(Iteration 3141 / 19600) loss: 1.073473\n",
      "(Iteration 3161 / 19600) loss: 0.937344\n",
      "(Iteration 3181 / 19600) loss: 1.120526\n",
      "(Iteration 3201 / 19600) loss: 1.279810\n",
      "(Iteration 3221 / 19600) loss: 1.058733\n",
      "(Iteration 3241 / 19600) loss: 1.146633\n",
      "(Iteration 3261 / 19600) loss: 1.007671\n",
      "(Iteration 3281 / 19600) loss: 1.022574\n",
      "(Iteration 3301 / 19600) loss: 1.266152\n",
      "(Iteration 3321 / 19600) loss: 0.963846\n",
      "(Iteration 3341 / 19600) loss: 1.324765\n",
      "(Iteration 3361 / 19600) loss: 1.162383\n",
      "(Iteration 3381 / 19600) loss: 1.221267\n",
      "(Iteration 3401 / 19600) loss: 0.848102\n",
      "(Iteration 3421 / 19600) loss: 1.214488\n",
      "(Iteration 3441 / 19600) loss: 0.959576\n",
      "(Iteration 3461 / 19600) loss: 0.934559\n",
      "(Iteration 3481 / 19600) loss: 1.000282\n",
      "(Iteration 3501 / 19600) loss: 1.088660\n",
      "(Iteration 3521 / 19600) loss: 0.833285\n",
      "(Iteration 3541 / 19600) loss: 1.087497\n",
      "(Iteration 3561 / 19600) loss: 1.010009\n",
      "(Iteration 3581 / 19600) loss: 1.101077\n",
      "(Iteration 3601 / 19600) loss: 1.180979\n",
      "(Iteration 3621 / 19600) loss: 1.196040\n",
      "(Iteration 3641 / 19600) loss: 0.928160\n",
      "(Iteration 3661 / 19600) loss: 1.006508\n",
      "(Iteration 3681 / 19600) loss: 0.981792\n",
      "(Iteration 3701 / 19600) loss: 1.095698\n",
      "(Iteration 3721 / 19600) loss: 0.925529\n",
      "(Iteration 3741 / 19600) loss: 1.071275\n",
      "(Iteration 3761 / 19600) loss: 1.014380\n",
      "(Iteration 3781 / 19600) loss: 0.889304\n",
      "(Iteration 3801 / 19600) loss: 0.844335\n",
      "(Iteration 3821 / 19600) loss: 1.262643\n",
      "(Iteration 3841 / 19600) loss: 1.079796\n",
      "(Iteration 3861 / 19600) loss: 0.986077\n",
      "(Iteration 3881 / 19600) loss: 1.144025\n",
      "(Iteration 3901 / 19600) loss: 1.002299\n",
      "(Epoch 4 / 20) train acc: 0.724000; val_acc: 0.605000\n",
      "(Iteration 3921 / 19600) loss: 1.047466\n",
      "(Iteration 3941 / 19600) loss: 1.135289\n",
      "(Iteration 3961 / 19600) loss: 1.133958\n",
      "(Iteration 3981 / 19600) loss: 1.154367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 4001 / 19600) loss: 1.167982\n",
      "(Iteration 4021 / 19600) loss: 0.884579\n",
      "(Iteration 4041 / 19600) loss: 1.058604\n",
      "(Iteration 4061 / 19600) loss: 1.029758\n",
      "(Iteration 4081 / 19600) loss: 0.986752\n",
      "(Iteration 4101 / 19600) loss: 1.041884\n",
      "(Iteration 4121 / 19600) loss: 0.770114\n",
      "(Iteration 4141 / 19600) loss: 1.002483\n",
      "(Iteration 4161 / 19600) loss: 1.008741\n",
      "(Iteration 4181 / 19600) loss: 0.895453\n",
      "(Iteration 4201 / 19600) loss: 1.259254\n",
      "(Iteration 4221 / 19600) loss: 1.108957\n",
      "(Iteration 4241 / 19600) loss: 1.057575\n",
      "(Iteration 4261 / 19600) loss: 0.977928\n",
      "(Iteration 4281 / 19600) loss: 0.896589\n",
      "(Iteration 4301 / 19600) loss: 0.869349\n",
      "(Iteration 4321 / 19600) loss: 1.073363\n",
      "(Iteration 4341 / 19600) loss: 0.884103\n",
      "(Iteration 4361 / 19600) loss: 1.237046\n",
      "(Iteration 4381 / 19600) loss: 0.773557\n",
      "(Iteration 4401 / 19600) loss: 0.999610\n",
      "(Iteration 4421 / 19600) loss: 1.062030\n",
      "(Iteration 4441 / 19600) loss: 0.692377\n",
      "(Iteration 4461 / 19600) loss: 0.799628\n",
      "(Iteration 4481 / 19600) loss: 0.789898\n",
      "(Iteration 4501 / 19600) loss: 1.094972\n",
      "(Iteration 4521 / 19600) loss: 0.875310\n",
      "(Iteration 4541 / 19600) loss: 1.092497\n",
      "(Iteration 4561 / 19600) loss: 1.022189\n",
      "(Iteration 4581 / 19600) loss: 0.980519\n",
      "(Iteration 4601 / 19600) loss: 1.091771\n",
      "(Iteration 4621 / 19600) loss: 1.116138\n",
      "(Iteration 4641 / 19600) loss: 0.804863\n",
      "(Iteration 4661 / 19600) loss: 0.679493\n",
      "(Iteration 4681 / 19600) loss: 0.908415\n",
      "(Iteration 4701 / 19600) loss: 1.005249\n",
      "(Iteration 4721 / 19600) loss: 0.920926\n",
      "(Iteration 4741 / 19600) loss: 0.867338\n",
      "(Iteration 4761 / 19600) loss: 1.073710\n",
      "(Iteration 4781 / 19600) loss: 1.369008\n",
      "(Iteration 4801 / 19600) loss: 0.920894\n",
      "(Iteration 4821 / 19600) loss: 1.059106\n",
      "(Iteration 4841 / 19600) loss: 0.864183\n",
      "(Iteration 4861 / 19600) loss: 1.011361\n",
      "(Iteration 4881 / 19600) loss: 0.994919\n",
      "(Epoch 5 / 20) train acc: 0.738000; val_acc: 0.585000\n",
      "(Iteration 4901 / 19600) loss: 1.070751\n",
      "(Iteration 4921 / 19600) loss: 0.955436\n",
      "(Iteration 4941 / 19600) loss: 0.897585\n",
      "(Iteration 4961 / 19600) loss: 0.839710\n",
      "(Iteration 4981 / 19600) loss: 0.746564\n",
      "(Iteration 5001 / 19600) loss: 1.044744\n",
      "(Iteration 5021 / 19600) loss: 0.989633\n",
      "(Iteration 5041 / 19600) loss: 0.976414\n",
      "(Iteration 5061 / 19600) loss: 0.753006\n",
      "(Iteration 5081 / 19600) loss: 1.103821\n",
      "(Iteration 5101 / 19600) loss: 1.090361\n",
      "(Iteration 5121 / 19600) loss: 0.970851\n",
      "(Iteration 5141 / 19600) loss: 0.844600\n",
      "(Iteration 5161 / 19600) loss: 0.710104\n",
      "(Iteration 5181 / 19600) loss: 0.692920\n",
      "(Iteration 5201 / 19600) loss: 0.841380\n",
      "(Iteration 5221 / 19600) loss: 0.960045\n",
      "(Iteration 5241 / 19600) loss: 0.879795\n",
      "(Iteration 5261 / 19600) loss: 0.725907\n",
      "(Iteration 5281 / 19600) loss: 0.939552\n",
      "(Iteration 5301 / 19600) loss: 0.987414\n",
      "(Iteration 5321 / 19600) loss: 1.009424\n",
      "(Iteration 5341 / 19600) loss: 0.900100\n",
      "(Iteration 5361 / 19600) loss: 0.978457\n",
      "(Iteration 5381 / 19600) loss: 1.111709\n",
      "(Iteration 5401 / 19600) loss: 0.843476\n",
      "(Iteration 5421 / 19600) loss: 0.934691\n",
      "(Iteration 5441 / 19600) loss: 0.838825\n",
      "(Iteration 5461 / 19600) loss: 1.145536\n",
      "(Iteration 5481 / 19600) loss: 0.729803\n",
      "(Iteration 5501 / 19600) loss: 0.949414\n",
      "(Iteration 5521 / 19600) loss: 1.116186\n",
      "(Iteration 5541 / 19600) loss: 1.089448\n",
      "(Iteration 5561 / 19600) loss: 0.959947\n",
      "(Iteration 5581 / 19600) loss: 1.105508\n",
      "(Iteration 5601 / 19600) loss: 0.869924\n",
      "(Iteration 5621 / 19600) loss: 1.145846\n",
      "(Iteration 5641 / 19600) loss: 1.080977\n",
      "(Iteration 5661 / 19600) loss: 1.005037\n",
      "(Iteration 5681 / 19600) loss: 1.037558\n",
      "(Iteration 5701 / 19600) loss: 0.758429\n",
      "(Iteration 5721 / 19600) loss: 0.965473\n",
      "(Iteration 5741 / 19600) loss: 0.945146\n",
      "(Iteration 5761 / 19600) loss: 0.852444\n",
      "(Iteration 5781 / 19600) loss: 0.842060\n",
      "(Iteration 5801 / 19600) loss: 0.656332\n",
      "(Iteration 5821 / 19600) loss: 0.740643\n",
      "(Iteration 5841 / 19600) loss: 0.985345\n",
      "(Iteration 5861 / 19600) loss: 1.078227\n",
      "(Epoch 6 / 20) train acc: 0.766000; val_acc: 0.583000\n",
      "(Iteration 5881 / 19600) loss: 0.850186\n",
      "(Iteration 5901 / 19600) loss: 0.607839\n",
      "(Iteration 5921 / 19600) loss: 0.716629\n",
      "(Iteration 5941 / 19600) loss: 0.887271\n",
      "(Iteration 5961 / 19600) loss: 1.161073\n",
      "(Iteration 5981 / 19600) loss: 0.772291\n",
      "(Iteration 6001 / 19600) loss: 0.892596\n",
      "(Iteration 6021 / 19600) loss: 0.888141\n",
      "(Iteration 6041 / 19600) loss: 0.656801\n",
      "(Iteration 6061 / 19600) loss: 0.931484\n",
      "(Iteration 6081 / 19600) loss: 1.118994\n",
      "(Iteration 6101 / 19600) loss: 1.017127\n",
      "(Iteration 6121 / 19600) loss: 0.933661\n",
      "(Iteration 6141 / 19600) loss: 0.822003\n",
      "(Iteration 6161 / 19600) loss: 1.027060\n",
      "(Iteration 6181 / 19600) loss: 0.873532\n",
      "(Iteration 6201 / 19600) loss: 0.931018\n",
      "(Iteration 6221 / 19600) loss: 0.728856\n",
      "(Iteration 6241 / 19600) loss: 0.896214\n",
      "(Iteration 6261 / 19600) loss: 0.684263\n",
      "(Iteration 6281 / 19600) loss: 0.941731\n",
      "(Iteration 6301 / 19600) loss: 0.794076\n",
      "(Iteration 6321 / 19600) loss: 0.722793\n",
      "(Iteration 6341 / 19600) loss: 0.833167\n",
      "(Iteration 6361 / 19600) loss: 0.924093\n",
      "(Iteration 6381 / 19600) loss: 0.727489\n",
      "(Iteration 6401 / 19600) loss: 1.315865\n",
      "(Iteration 6421 / 19600) loss: 1.086097\n",
      "(Iteration 6441 / 19600) loss: 0.859197\n",
      "(Iteration 6461 / 19600) loss: 1.129385\n",
      "(Iteration 6481 / 19600) loss: 0.912302\n",
      "(Iteration 6501 / 19600) loss: 0.887663\n",
      "(Iteration 6521 / 19600) loss: 0.771798\n",
      "(Iteration 6541 / 19600) loss: 1.069130\n",
      "(Iteration 6561 / 19600) loss: 1.018504\n",
      "(Iteration 6581 / 19600) loss: 0.814869\n",
      "(Iteration 6601 / 19600) loss: 0.747343\n",
      "(Iteration 6621 / 19600) loss: 0.825286\n",
      "(Iteration 6641 / 19600) loss: 0.913499\n",
      "(Iteration 6661 / 19600) loss: 0.864824\n",
      "(Iteration 6681 / 19600) loss: 1.129961\n",
      "(Iteration 6701 / 19600) loss: 1.128044\n",
      "(Iteration 6721 / 19600) loss: 0.699514\n",
      "(Iteration 6741 / 19600) loss: 0.668527\n",
      "(Iteration 6761 / 19600) loss: 0.892610\n",
      "(Iteration 6781 / 19600) loss: 0.752453\n",
      "(Iteration 6801 / 19600) loss: 0.984267\n",
      "(Iteration 6821 / 19600) loss: 0.866194\n",
      "(Iteration 6841 / 19600) loss: 1.119861\n",
      "(Epoch 7 / 20) train acc: 0.760000; val_acc: 0.591000\n",
      "(Iteration 6861 / 19600) loss: 0.742989\n",
      "(Iteration 6881 / 19600) loss: 0.867092\n",
      "(Iteration 6901 / 19600) loss: 0.771467\n",
      "(Iteration 6921 / 19600) loss: 0.751122\n",
      "(Iteration 6941 / 19600) loss: 1.029397\n",
      "(Iteration 6961 / 19600) loss: 0.970587\n",
      "(Iteration 6981 / 19600) loss: 0.934854\n",
      "(Iteration 7001 / 19600) loss: 0.807292\n",
      "(Iteration 7021 / 19600) loss: 0.743636\n",
      "(Iteration 7041 / 19600) loss: 0.533022\n",
      "(Iteration 7061 / 19600) loss: 0.743381\n",
      "(Iteration 7081 / 19600) loss: 0.810689\n",
      "(Iteration 7101 / 19600) loss: 0.822262\n",
      "(Iteration 7121 / 19600) loss: 0.872967\n",
      "(Iteration 7141 / 19600) loss: 1.262770\n",
      "(Iteration 7161 / 19600) loss: 0.740145\n",
      "(Iteration 7181 / 19600) loss: 1.037604\n",
      "(Iteration 7201 / 19600) loss: 0.606011\n",
      "(Iteration 7221 / 19600) loss: 1.099697\n",
      "(Iteration 7241 / 19600) loss: 0.806133\n",
      "(Iteration 7261 / 19600) loss: 0.801057\n",
      "(Iteration 7281 / 19600) loss: 1.065509\n",
      "(Iteration 7301 / 19600) loss: 0.901616\n",
      "(Iteration 7321 / 19600) loss: 0.956063\n",
      "(Iteration 7341 / 19600) loss: 1.222251\n",
      "(Iteration 7361 / 19600) loss: 0.685210\n",
      "(Iteration 7381 / 19600) loss: 0.751363\n",
      "(Iteration 7401 / 19600) loss: 0.802162\n",
      "(Iteration 7421 / 19600) loss: 0.973041\n",
      "(Iteration 7441 / 19600) loss: 0.699086\n",
      "(Iteration 7461 / 19600) loss: 0.898768\n",
      "(Iteration 7481 / 19600) loss: 1.024307\n",
      "(Iteration 7501 / 19600) loss: 0.614787\n",
      "(Iteration 7521 / 19600) loss: 0.739637\n",
      "(Iteration 7541 / 19600) loss: 0.930498\n",
      "(Iteration 7561 / 19600) loss: 0.975102\n",
      "(Iteration 7581 / 19600) loss: 0.770981\n",
      "(Iteration 7601 / 19600) loss: 0.793030\n",
      "(Iteration 7621 / 19600) loss: 1.144594\n",
      "(Iteration 7641 / 19600) loss: 1.023673\n",
      "(Iteration 7661 / 19600) loss: 0.943658\n",
      "(Iteration 7681 / 19600) loss: 0.727383\n",
      "(Iteration 7701 / 19600) loss: 1.059930\n",
      "(Iteration 7721 / 19600) loss: 0.731219\n",
      "(Iteration 7741 / 19600) loss: 0.700718\n",
      "(Iteration 7761 / 19600) loss: 0.864005\n",
      "(Iteration 7781 / 19600) loss: 0.944532\n",
      "(Iteration 7801 / 19600) loss: 1.122444\n",
      "(Iteration 7821 / 19600) loss: 0.955182\n",
      "(Epoch 8 / 20) train acc: 0.795000; val_acc: 0.601000\n",
      "(Iteration 7841 / 19600) loss: 0.688640\n",
      "(Iteration 7861 / 19600) loss: 0.798893\n",
      "(Iteration 7881 / 19600) loss: 0.752816\n",
      "(Iteration 7901 / 19600) loss: 0.800965\n",
      "(Iteration 7921 / 19600) loss: 0.727316\n",
      "(Iteration 7941 / 19600) loss: 0.856195\n",
      "(Iteration 7961 / 19600) loss: 0.919387\n",
      "(Iteration 7981 / 19600) loss: 0.934722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 8001 / 19600) loss: 0.583386\n",
      "(Iteration 8021 / 19600) loss: 0.696836\n",
      "(Iteration 8041 / 19600) loss: 0.881706\n",
      "(Iteration 8061 / 19600) loss: 0.727679\n",
      "(Iteration 8081 / 19600) loss: 0.863285\n",
      "(Iteration 8101 / 19600) loss: 0.736205\n",
      "(Iteration 8121 / 19600) loss: 0.910430\n",
      "(Iteration 8141 / 19600) loss: 1.084528\n",
      "(Iteration 8161 / 19600) loss: 1.036334\n",
      "(Iteration 8181 / 19600) loss: 0.800576\n",
      "(Iteration 8201 / 19600) loss: 0.949233\n",
      "(Iteration 8221 / 19600) loss: 0.725026\n",
      "(Iteration 8241 / 19600) loss: 0.829444\n",
      "(Iteration 8261 / 19600) loss: 0.843619\n",
      "(Iteration 8281 / 19600) loss: 0.806297\n",
      "(Iteration 8301 / 19600) loss: 0.874344\n",
      "(Iteration 8321 / 19600) loss: 0.847152\n",
      "(Iteration 8341 / 19600) loss: 0.927289\n",
      "(Iteration 8361 / 19600) loss: 0.784617\n",
      "(Iteration 8381 / 19600) loss: 0.716128\n",
      "(Iteration 8401 / 19600) loss: 0.775639\n",
      "(Iteration 8421 / 19600) loss: 0.947131\n",
      "(Iteration 8441 / 19600) loss: 0.787036\n",
      "(Iteration 8461 / 19600) loss: 0.982163\n",
      "(Iteration 8481 / 19600) loss: 0.949378\n",
      "(Iteration 8501 / 19600) loss: 0.947626\n",
      "(Iteration 8521 / 19600) loss: 0.796228\n",
      "(Iteration 8541 / 19600) loss: 0.700301\n",
      "(Iteration 8561 / 19600) loss: 0.708418\n",
      "(Iteration 8581 / 19600) loss: 0.994562\n",
      "(Iteration 8601 / 19600) loss: 0.989834\n",
      "(Iteration 8621 / 19600) loss: 0.988725\n",
      "(Iteration 8641 / 19600) loss: 0.787133\n",
      "(Iteration 8661 / 19600) loss: 0.643757\n",
      "(Iteration 8681 / 19600) loss: 0.908507\n",
      "(Iteration 8701 / 19600) loss: 1.127185\n",
      "(Iteration 8721 / 19600) loss: 0.796949\n",
      "(Iteration 8741 / 19600) loss: 0.612477\n",
      "(Iteration 8761 / 19600) loss: 0.699128\n",
      "(Iteration 8781 / 19600) loss: 0.922675\n",
      "(Iteration 8801 / 19600) loss: 0.689302\n",
      "(Epoch 9 / 20) train acc: 0.837000; val_acc: 0.594000\n",
      "(Iteration 8821 / 19600) loss: 0.676580\n",
      "(Iteration 8841 / 19600) loss: 0.602202\n",
      "(Iteration 8861 / 19600) loss: 0.935766\n",
      "(Iteration 8881 / 19600) loss: 0.966083\n",
      "(Iteration 8901 / 19600) loss: 0.887616\n",
      "(Iteration 8921 / 19600) loss: 0.702744\n",
      "(Iteration 8941 / 19600) loss: 0.548441\n",
      "(Iteration 8961 / 19600) loss: 0.771913\n",
      "(Iteration 8981 / 19600) loss: 0.669075\n",
      "(Iteration 9001 / 19600) loss: 0.655699\n",
      "(Iteration 9021 / 19600) loss: 0.590233\n",
      "(Iteration 9041 / 19600) loss: 0.744507\n",
      "(Iteration 9061 / 19600) loss: 0.691308\n",
      "(Iteration 9081 / 19600) loss: 0.878150\n",
      "(Iteration 9101 / 19600) loss: 0.911839\n",
      "(Iteration 9121 / 19600) loss: 0.806354\n",
      "(Iteration 9141 / 19600) loss: 0.794170\n",
      "(Iteration 9161 / 19600) loss: 0.835946\n",
      "(Iteration 9181 / 19600) loss: 0.690383\n",
      "(Iteration 9201 / 19600) loss: 0.766181\n",
      "(Iteration 9221 / 19600) loss: 0.701436\n",
      "(Iteration 9241 / 19600) loss: 0.677653\n",
      "(Iteration 9261 / 19600) loss: 1.015700\n",
      "(Iteration 9281 / 19600) loss: 0.784734\n",
      "(Iteration 9301 / 19600) loss: 0.660899\n",
      "(Iteration 9321 / 19600) loss: 0.683145\n",
      "(Iteration 9341 / 19600) loss: 0.732838\n",
      "(Iteration 9361 / 19600) loss: 0.686045\n",
      "(Iteration 9381 / 19600) loss: 0.931031\n",
      "(Iteration 9401 / 19600) loss: 1.091101\n",
      "(Iteration 9421 / 19600) loss: 0.789632\n",
      "(Iteration 9441 / 19600) loss: 0.579501\n",
      "(Iteration 9461 / 19600) loss: 0.935871\n",
      "(Iteration 9481 / 19600) loss: 0.815304\n",
      "(Iteration 9501 / 19600) loss: 1.011770\n",
      "(Iteration 9521 / 19600) loss: 0.672777\n",
      "(Iteration 9541 / 19600) loss: 0.755877\n",
      "(Iteration 9561 / 19600) loss: 0.555377\n",
      "(Iteration 9581 / 19600) loss: 0.864626\n",
      "(Iteration 9601 / 19600) loss: 0.642203\n",
      "(Iteration 9621 / 19600) loss: 0.813512\n",
      "(Iteration 9641 / 19600) loss: 0.976563\n",
      "(Iteration 9661 / 19600) loss: 0.779142\n",
      "(Iteration 9681 / 19600) loss: 0.828895\n",
      "(Iteration 9701 / 19600) loss: 0.706187\n",
      "(Iteration 9721 / 19600) loss: 0.681086\n",
      "(Iteration 9741 / 19600) loss: 0.957203\n",
      "(Iteration 9761 / 19600) loss: 0.501601\n",
      "(Iteration 9781 / 19600) loss: 0.755907\n",
      "(Epoch 10 / 20) train acc: 0.858000; val_acc: 0.602000\n",
      "(Iteration 9801 / 19600) loss: 0.575857\n",
      "(Iteration 9821 / 19600) loss: 0.693969\n",
      "(Iteration 9841 / 19600) loss: 0.718479\n",
      "(Iteration 9861 / 19600) loss: 0.651233\n",
      "(Iteration 9881 / 19600) loss: 0.687203\n",
      "(Iteration 9901 / 19600) loss: 0.741520\n",
      "(Iteration 9921 / 19600) loss: 0.644386\n",
      "(Iteration 9941 / 19600) loss: 0.627076\n",
      "(Iteration 9961 / 19600) loss: 0.743641\n",
      "(Iteration 9981 / 19600) loss: 0.924088\n",
      "(Iteration 10001 / 19600) loss: 0.621082\n",
      "(Iteration 10021 / 19600) loss: 0.658815\n",
      "(Iteration 10041 / 19600) loss: 0.656260\n",
      "(Iteration 10061 / 19600) loss: 0.920259\n",
      "(Iteration 10081 / 19600) loss: 0.666396\n",
      "(Iteration 10101 / 19600) loss: 0.751735\n",
      "(Iteration 10121 / 19600) loss: 0.904521\n",
      "(Iteration 10141 / 19600) loss: 0.499334\n",
      "(Iteration 10161 / 19600) loss: 0.773539\n",
      "(Iteration 10181 / 19600) loss: 0.890255\n",
      "(Iteration 10201 / 19600) loss: 0.765020\n",
      "(Iteration 10221 / 19600) loss: 0.531027\n",
      "(Iteration 10241 / 19600) loss: 0.763408\n",
      "(Iteration 10261 / 19600) loss: 1.071979\n",
      "(Iteration 10281 / 19600) loss: 1.139486\n",
      "(Iteration 10301 / 19600) loss: 0.782801\n",
      "(Iteration 10321 / 19600) loss: 0.757350\n",
      "(Iteration 10341 / 19600) loss: 0.547560\n",
      "(Iteration 10361 / 19600) loss: 0.708732\n",
      "(Iteration 10381 / 19600) loss: 1.313110\n",
      "(Iteration 10401 / 19600) loss: 0.798168\n",
      "(Iteration 10421 / 19600) loss: 0.800542\n",
      "(Iteration 10441 / 19600) loss: 0.691393\n",
      "(Iteration 10461 / 19600) loss: 0.779757\n",
      "(Iteration 10481 / 19600) loss: 0.854961\n",
      "(Iteration 10501 / 19600) loss: 0.982582\n",
      "(Iteration 10521 / 19600) loss: 0.961377\n",
      "(Iteration 10541 / 19600) loss: 0.626926\n",
      "(Iteration 10561 / 19600) loss: 0.760795\n",
      "(Iteration 10581 / 19600) loss: 0.655320\n",
      "(Iteration 10601 / 19600) loss: 0.664311\n",
      "(Iteration 10621 / 19600) loss: 0.833392\n",
      "(Iteration 10641 / 19600) loss: 0.771845\n",
      "(Iteration 10661 / 19600) loss: 0.812424\n",
      "(Iteration 10681 / 19600) loss: 0.745343\n",
      "(Iteration 10701 / 19600) loss: 0.783671\n",
      "(Iteration 10721 / 19600) loss: 0.721855\n",
      "(Iteration 10741 / 19600) loss: 0.731709\n",
      "(Iteration 10761 / 19600) loss: 0.716377\n",
      "(Epoch 11 / 20) train acc: 0.835000; val_acc: 0.616000\n",
      "(Iteration 10781 / 19600) loss: 0.550346\n",
      "(Iteration 10801 / 19600) loss: 0.794009\n",
      "(Iteration 10821 / 19600) loss: 0.848650\n",
      "(Iteration 10841 / 19600) loss: 0.866442\n",
      "(Iteration 10861 / 19600) loss: 0.628663\n",
      "(Iteration 10881 / 19600) loss: 0.647719\n",
      "(Iteration 10901 / 19600) loss: 0.736066\n",
      "(Iteration 10921 / 19600) loss: 0.889426\n",
      "(Iteration 10941 / 19600) loss: 0.630810\n",
      "(Iteration 10961 / 19600) loss: 0.972582\n",
      "(Iteration 10981 / 19600) loss: 0.574976\n",
      "(Iteration 11001 / 19600) loss: 0.660481\n",
      "(Iteration 11021 / 19600) loss: 0.745139\n",
      "(Iteration 11041 / 19600) loss: 0.928584\n",
      "(Iteration 11061 / 19600) loss: 0.736292\n",
      "(Iteration 11081 / 19600) loss: 0.787722\n",
      "(Iteration 11101 / 19600) loss: 0.824583\n",
      "(Iteration 11121 / 19600) loss: 0.554119\n",
      "(Iteration 11141 / 19600) loss: 0.715626\n",
      "(Iteration 11161 / 19600) loss: 0.792837\n",
      "(Iteration 11181 / 19600) loss: 0.803239\n",
      "(Iteration 11201 / 19600) loss: 1.165002\n",
      "(Iteration 11221 / 19600) loss: 1.277482\n",
      "(Iteration 11241 / 19600) loss: 0.810360\n",
      "(Iteration 11261 / 19600) loss: 0.916125\n",
      "(Iteration 11281 / 19600) loss: 0.741876\n",
      "(Iteration 11301 / 19600) loss: 0.726728\n",
      "(Iteration 11321 / 19600) loss: 0.987485\n",
      "(Iteration 11341 / 19600) loss: 0.613189\n",
      "(Iteration 11361 / 19600) loss: 1.012265\n",
      "(Iteration 11381 / 19600) loss: 0.562206\n",
      "(Iteration 11401 / 19600) loss: 0.887584\n",
      "(Iteration 11421 / 19600) loss: 1.064119\n",
      "(Iteration 11441 / 19600) loss: 0.797571\n",
      "(Iteration 11461 / 19600) loss: 1.006817\n",
      "(Iteration 11481 / 19600) loss: 0.694173\n",
      "(Iteration 11501 / 19600) loss: 0.642585\n",
      "(Iteration 11521 / 19600) loss: 0.695860\n",
      "(Iteration 11541 / 19600) loss: 0.682068\n",
      "(Iteration 11561 / 19600) loss: 0.573959\n",
      "(Iteration 11581 / 19600) loss: 0.818288\n",
      "(Iteration 11601 / 19600) loss: 1.173211\n",
      "(Iteration 11621 / 19600) loss: 0.735389\n",
      "(Iteration 11641 / 19600) loss: 0.837792\n",
      "(Iteration 11661 / 19600) loss: 1.001505\n",
      "(Iteration 11681 / 19600) loss: 0.570190\n",
      "(Iteration 11701 / 19600) loss: 0.618553\n",
      "(Iteration 11721 / 19600) loss: 0.817039\n",
      "(Iteration 11741 / 19600) loss: 0.640346\n",
      "(Epoch 12 / 20) train acc: 0.834000; val_acc: 0.585000\n",
      "(Iteration 11761 / 19600) loss: 0.689636\n",
      "(Iteration 11781 / 19600) loss: 0.702041\n",
      "(Iteration 11801 / 19600) loss: 0.843529\n",
      "(Iteration 11821 / 19600) loss: 0.891026\n",
      "(Iteration 11841 / 19600) loss: 0.697861\n",
      "(Iteration 11861 / 19600) loss: 0.592328\n",
      "(Iteration 11881 / 19600) loss: 0.703555\n",
      "(Iteration 11901 / 19600) loss: 0.608084\n",
      "(Iteration 11921 / 19600) loss: 0.702759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 11941 / 19600) loss: 0.737043\n",
      "(Iteration 11961 / 19600) loss: 0.900304\n",
      "(Iteration 11981 / 19600) loss: 0.762288\n",
      "(Iteration 12001 / 19600) loss: 0.824412\n",
      "(Iteration 12021 / 19600) loss: 0.890213\n",
      "(Iteration 12041 / 19600) loss: 0.785258\n",
      "(Iteration 12061 / 19600) loss: 0.653531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1707d72e0710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 },\n\u001b[1;32m      9\u001b[0m                 verbose=True, print_every=20)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfull_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/weave_convolution/solver.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Maybe print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/weave_convolution/solver.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# Compute loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/weave_convolution/cnn.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m#Large convu layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtheta_large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtheta_large_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_relu_pool_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_large_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'theta_large'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtheta_large\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_large\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'theta_large_0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtheta_large_0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/weave_convolution/layer_utils.pyc\u001b[0m in \u001b[0;36mconv_relu_pool_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_pool_backward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0mda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m   \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_backward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tim/Documents/GitHub/weave_convolution/fast_layers.pyc\u001b[0m in \u001b[0;36mconv_backward_strides\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0mdx_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mdx_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol2im_6d_cython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_full = ThreeLayerFancyNet(weight_scale=0.001, hidden_dim=100, reg=0.001)\n",
    "\n",
    "full_solver = Solver(model_full, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "full_solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 19600) loss: 2.304658\n",
      "(Epoch 0 / 20) train acc: 0.099000; val_acc: 0.087000\n",
      "(Iteration 21 / 19600) loss: 2.036802\n",
      "(Iteration 41 / 19600) loss: 2.056274\n",
      "(Iteration 61 / 19600) loss: 2.221853\n",
      "(Iteration 81 / 19600) loss: 1.858100\n",
      "(Iteration 101 / 19600) loss: 1.890737\n",
      "(Iteration 121 / 19600) loss: 1.930386\n",
      "(Iteration 141 / 19600) loss: 1.907577\n",
      "(Iteration 161 / 19600) loss: 1.814935\n",
      "(Iteration 181 / 19600) loss: 1.880776\n",
      "(Iteration 201 / 19600) loss: 1.769854\n",
      "(Iteration 221 / 19600) loss: 1.766677\n",
      "(Iteration 241 / 19600) loss: 1.578168\n",
      "(Iteration 261 / 19600) loss: 1.850327\n",
      "(Iteration 281 / 19600) loss: 1.767173\n",
      "(Iteration 301 / 19600) loss: 1.605269\n",
      "(Iteration 321 / 19600) loss: 1.665006\n",
      "(Iteration 341 / 19600) loss: 1.684240\n",
      "(Iteration 361 / 19600) loss: 1.552680\n",
      "(Iteration 381 / 19600) loss: 1.706762\n",
      "(Iteration 401 / 19600) loss: 1.907821\n",
      "(Iteration 421 / 19600) loss: 1.205393\n",
      "(Iteration 441 / 19600) loss: 1.570936\n",
      "(Iteration 461 / 19600) loss: 1.455121\n",
      "(Iteration 481 / 19600) loss: 1.642012\n",
      "(Iteration 501 / 19600) loss: 1.625800\n",
      "(Iteration 521 / 19600) loss: 1.586624\n",
      "(Iteration 541 / 19600) loss: 1.416176\n",
      "(Iteration 561 / 19600) loss: 1.644400\n",
      "(Iteration 581 / 19600) loss: 1.679138\n",
      "(Iteration 601 / 19600) loss: 1.974947\n",
      "(Iteration 621 / 19600) loss: 1.238145\n",
      "(Iteration 641 / 19600) loss: 1.713012\n",
      "(Iteration 661 / 19600) loss: 1.371846\n",
      "(Iteration 681 / 19600) loss: 1.533012\n",
      "(Iteration 701 / 19600) loss: 1.470856\n",
      "(Iteration 721 / 19600) loss: 1.663051\n",
      "(Iteration 741 / 19600) loss: 1.520187\n",
      "(Iteration 761 / 19600) loss: 1.278627\n",
      "(Iteration 781 / 19600) loss: 1.528351\n",
      "(Iteration 801 / 19600) loss: 1.583394\n",
      "(Iteration 821 / 19600) loss: 1.563086\n",
      "(Iteration 841 / 19600) loss: 1.391517\n",
      "(Iteration 861 / 19600) loss: 1.892688\n",
      "(Iteration 881 / 19600) loss: 1.562932\n",
      "(Iteration 901 / 19600) loss: 1.502870\n",
      "(Iteration 921 / 19600) loss: 1.593970\n",
      "(Iteration 941 / 19600) loss: 1.589390\n",
      "(Iteration 961 / 19600) loss: 1.533474\n",
      "(Epoch 1 / 20) train acc: 0.529000; val_acc: 0.520000\n",
      "(Iteration 981 / 19600) loss: 1.574034\n",
      "(Iteration 1001 / 19600) loss: 1.479984\n",
      "(Iteration 1021 / 19600) loss: 1.547364\n",
      "(Iteration 1041 / 19600) loss: 1.657546\n",
      "(Iteration 1061 / 19600) loss: 1.552517\n",
      "(Iteration 1081 / 19600) loss: 1.448122\n",
      "(Iteration 1101 / 19600) loss: 1.259334\n",
      "(Iteration 1121 / 19600) loss: 1.560281\n",
      "(Iteration 1141 / 19600) loss: 1.190638\n",
      "(Iteration 1161 / 19600) loss: 1.745847\n",
      "(Iteration 1181 / 19600) loss: 1.595119\n",
      "(Iteration 1201 / 19600) loss: 1.523107\n",
      "(Iteration 1221 / 19600) loss: 1.497919\n",
      "(Iteration 1241 / 19600) loss: 1.452257\n",
      "(Iteration 1261 / 19600) loss: 1.501592\n",
      "(Iteration 1281 / 19600) loss: 1.325694\n",
      "(Iteration 1301 / 19600) loss: 1.285206\n",
      "(Iteration 1321 / 19600) loss: 1.360993\n",
      "(Iteration 1341 / 19600) loss: 1.426935\n",
      "(Iteration 1361 / 19600) loss: 1.500239\n",
      "(Iteration 1381 / 19600) loss: 1.489436\n",
      "(Iteration 1401 / 19600) loss: 1.326032\n",
      "(Iteration 1421 / 19600) loss: 1.410458\n",
      "(Iteration 1441 / 19600) loss: 1.060467\n",
      "(Iteration 1461 / 19600) loss: 1.244090\n",
      "(Iteration 1481 / 19600) loss: 1.529173\n",
      "(Iteration 1501 / 19600) loss: 1.677130\n",
      "(Iteration 1521 / 19600) loss: 1.408044\n",
      "(Iteration 1541 / 19600) loss: 1.330607\n",
      "(Iteration 1561 / 19600) loss: 1.467074\n",
      "(Iteration 1581 / 19600) loss: 1.475867\n",
      "(Iteration 1601 / 19600) loss: 1.604201\n",
      "(Iteration 1621 / 19600) loss: 1.325220\n",
      "(Iteration 1641 / 19600) loss: 1.462543\n",
      "(Iteration 1661 / 19600) loss: 1.325531\n",
      "(Iteration 1681 / 19600) loss: 1.549006\n",
      "(Iteration 1701 / 19600) loss: 1.369535\n",
      "(Iteration 1721 / 19600) loss: 1.200758\n",
      "(Iteration 1741 / 19600) loss: 1.347677\n",
      "(Iteration 1761 / 19600) loss: 1.259920\n",
      "(Iteration 1781 / 19600) loss: 1.152279\n",
      "(Iteration 1801 / 19600) loss: 1.311993\n",
      "(Iteration 1821 / 19600) loss: 1.549076\n",
      "(Iteration 1841 / 19600) loss: 1.111827\n",
      "(Iteration 1861 / 19600) loss: 1.323767\n",
      "(Iteration 1881 / 19600) loss: 1.157818\n",
      "(Iteration 1901 / 19600) loss: 1.290394\n",
      "(Iteration 1921 / 19600) loss: 1.364666\n",
      "(Iteration 1941 / 19600) loss: 1.455637\n",
      "(Epoch 2 / 20) train acc: 0.580000; val_acc: 0.574000\n",
      "(Iteration 1961 / 19600) loss: 1.118595\n",
      "(Iteration 1981 / 19600) loss: 1.220707\n",
      "(Iteration 2001 / 19600) loss: 1.261483\n",
      "(Iteration 2021 / 19600) loss: 1.481724\n",
      "(Iteration 2041 / 19600) loss: 1.243415\n",
      "(Iteration 2061 / 19600) loss: 1.277861\n",
      "(Iteration 2081 / 19600) loss: 1.704741\n",
      "(Iteration 2101 / 19600) loss: 0.964216\n",
      "(Iteration 2121 / 19600) loss: 1.346056\n",
      "(Iteration 2141 / 19600) loss: 1.133196\n",
      "(Iteration 2161 / 19600) loss: 1.446764\n",
      "(Iteration 2181 / 19600) loss: 1.521700\n",
      "(Iteration 2201 / 19600) loss: 1.660457\n",
      "(Iteration 2221 / 19600) loss: 1.150125\n",
      "(Iteration 2241 / 19600) loss: 1.081222\n",
      "(Iteration 2261 / 19600) loss: 1.314135\n",
      "(Iteration 2281 / 19600) loss: 1.255551\n",
      "(Iteration 2301 / 19600) loss: 1.478589\n",
      "(Iteration 2321 / 19600) loss: 1.662770\n",
      "(Iteration 2341 / 19600) loss: 1.499222\n",
      "(Iteration 2361 / 19600) loss: 0.994836\n",
      "(Iteration 2381 / 19600) loss: 1.535084\n",
      "(Iteration 2401 / 19600) loss: 1.376731\n",
      "(Iteration 2421 / 19600) loss: 1.406933\n",
      "(Iteration 2441 / 19600) loss: 0.823446\n",
      "(Iteration 2461 / 19600) loss: 1.198433\n",
      "(Iteration 2481 / 19600) loss: 1.274735\n",
      "(Iteration 2501 / 19600) loss: 1.214594\n",
      "(Iteration 2521 / 19600) loss: 1.551707\n",
      "(Iteration 2541 / 19600) loss: 1.458195\n",
      "(Iteration 2561 / 19600) loss: 1.147767\n",
      "(Iteration 2581 / 19600) loss: 1.368233\n",
      "(Iteration 2601 / 19600) loss: 1.276450\n",
      "(Iteration 2621 / 19600) loss: 1.039488\n",
      "(Iteration 2641 / 19600) loss: 1.388012\n",
      "(Iteration 2661 / 19600) loss: 1.351347\n",
      "(Iteration 2681 / 19600) loss: 1.258380\n",
      "(Iteration 2701 / 19600) loss: 1.305130\n",
      "(Iteration 2721 / 19600) loss: 0.989812\n",
      "(Iteration 2741 / 19600) loss: 1.254801\n",
      "(Iteration 2761 / 19600) loss: 1.689267\n",
      "(Iteration 2781 / 19600) loss: 0.996971\n",
      "(Iteration 2801 / 19600) loss: 1.496442\n",
      "(Iteration 2821 / 19600) loss: 1.276679\n",
      "(Iteration 2841 / 19600) loss: 1.246807\n",
      "(Iteration 2861 / 19600) loss: 1.334338\n",
      "(Iteration 2881 / 19600) loss: 1.319507\n",
      "(Iteration 2901 / 19600) loss: 1.277505\n",
      "(Iteration 2921 / 19600) loss: 1.269150\n",
      "(Epoch 3 / 20) train acc: 0.628000; val_acc: 0.556000\n",
      "(Iteration 2941 / 19600) loss: 1.540823\n",
      "(Iteration 2961 / 19600) loss: 1.360839\n",
      "(Iteration 2981 / 19600) loss: 1.113704\n",
      "(Iteration 3001 / 19600) loss: 1.148970\n",
      "(Iteration 3021 / 19600) loss: 1.136868\n",
      "(Iteration 3041 / 19600) loss: 1.477360\n",
      "(Iteration 3061 / 19600) loss: 1.537149\n",
      "(Iteration 3081 / 19600) loss: 1.224790\n",
      "(Iteration 3101 / 19600) loss: 1.107904\n",
      "(Iteration 3121 / 19600) loss: 1.097183\n",
      "(Iteration 3141 / 19600) loss: 1.368209\n",
      "(Iteration 3161 / 19600) loss: 1.036824\n",
      "(Iteration 3181 / 19600) loss: 1.221880\n",
      "(Iteration 3201 / 19600) loss: 1.169219\n",
      "(Iteration 3221 / 19600) loss: 1.326707\n",
      "(Iteration 3241 / 19600) loss: 1.347840\n",
      "(Iteration 3261 / 19600) loss: 1.485296\n",
      "(Iteration 3281 / 19600) loss: 1.223784\n",
      "(Iteration 3301 / 19600) loss: 1.181047\n",
      "(Iteration 3321 / 19600) loss: 1.312831\n",
      "(Iteration 3341 / 19600) loss: 1.396255\n",
      "(Iteration 3361 / 19600) loss: 1.094473\n",
      "(Iteration 3381 / 19600) loss: 1.230625\n",
      "(Iteration 3401 / 19600) loss: 1.062286\n",
      "(Iteration 3421 / 19600) loss: 1.319455\n",
      "(Iteration 3441 / 19600) loss: 1.483950\n",
      "(Iteration 3461 / 19600) loss: 1.155700\n",
      "(Iteration 3481 / 19600) loss: 1.198271\n",
      "(Iteration 3501 / 19600) loss: 1.497005\n",
      "(Iteration 3521 / 19600) loss: 1.159228\n",
      "(Iteration 3541 / 19600) loss: 0.984003\n",
      "(Iteration 3561 / 19600) loss: 1.449090\n",
      "(Iteration 3581 / 19600) loss: 1.204812\n",
      "(Iteration 3601 / 19600) loss: 1.175881\n",
      "(Iteration 3621 / 19600) loss: 1.148781\n",
      "(Iteration 3641 / 19600) loss: 1.009355\n",
      "(Iteration 3661 / 19600) loss: 1.124075\n",
      "(Iteration 3681 / 19600) loss: 0.841537\n",
      "(Iteration 3701 / 19600) loss: 1.039165\n",
      "(Iteration 3721 / 19600) loss: 1.471611\n",
      "(Iteration 3741 / 19600) loss: 1.298119\n",
      "(Iteration 3761 / 19600) loss: 1.345087\n",
      "(Iteration 3781 / 19600) loss: 1.411189\n",
      "(Iteration 3801 / 19600) loss: 1.078119\n",
      "(Iteration 3821 / 19600) loss: 1.155457\n",
      "(Iteration 3841 / 19600) loss: 1.417242\n",
      "(Iteration 3861 / 19600) loss: 1.274828\n",
      "(Iteration 3881 / 19600) loss: 1.370286\n",
      "(Iteration 3901 / 19600) loss: 1.099674\n",
      "(Epoch 4 / 20) train acc: 0.649000; val_acc: 0.560000\n",
      "(Iteration 3921 / 19600) loss: 1.001969\n",
      "(Iteration 3941 / 19600) loss: 1.237077\n",
      "(Iteration 3961 / 19600) loss: 1.127584\n",
      "(Iteration 3981 / 19600) loss: 1.099561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 4001 / 19600) loss: 1.198526\n",
      "(Iteration 4021 / 19600) loss: 1.090084\n",
      "(Iteration 4041 / 19600) loss: 1.157909\n",
      "(Iteration 4061 / 19600) loss: 1.266726\n",
      "(Iteration 4081 / 19600) loss: 1.177557\n",
      "(Iteration 4101 / 19600) loss: 1.085612\n",
      "(Iteration 4121 / 19600) loss: 1.309049\n",
      "(Iteration 4141 / 19600) loss: 1.033016\n",
      "(Iteration 4161 / 19600) loss: 1.187502\n",
      "(Iteration 4181 / 19600) loss: 1.216729\n",
      "(Iteration 4201 / 19600) loss: 1.363861\n",
      "(Iteration 4221 / 19600) loss: 1.358580\n",
      "(Iteration 4241 / 19600) loss: 1.340638\n",
      "(Iteration 4261 / 19600) loss: 1.275264\n",
      "(Iteration 4281 / 19600) loss: 0.983631\n",
      "(Iteration 4301 / 19600) loss: 1.287950\n",
      "(Iteration 4321 / 19600) loss: 1.209719\n",
      "(Iteration 4341 / 19600) loss: 1.193875\n",
      "(Iteration 4361 / 19600) loss: 1.247742\n",
      "(Iteration 4381 / 19600) loss: 1.198375\n",
      "(Iteration 4401 / 19600) loss: 1.349892\n",
      "(Iteration 4421 / 19600) loss: 1.314110\n",
      "(Iteration 4441 / 19600) loss: 0.998195\n",
      "(Iteration 4461 / 19600) loss: 1.357110\n",
      "(Iteration 4481 / 19600) loss: 1.067838\n",
      "(Iteration 4501 / 19600) loss: 1.226737\n",
      "(Iteration 4521 / 19600) loss: 1.175242\n",
      "(Iteration 4541 / 19600) loss: 1.177787\n",
      "(Iteration 4561 / 19600) loss: 1.285970\n",
      "(Iteration 4581 / 19600) loss: 1.466038\n",
      "(Iteration 4601 / 19600) loss: 1.230630\n",
      "(Iteration 4621 / 19600) loss: 1.096513\n",
      "(Iteration 4641 / 19600) loss: 1.190563\n",
      "(Iteration 4661 / 19600) loss: 1.126532\n",
      "(Iteration 4681 / 19600) loss: 1.180400\n",
      "(Iteration 4701 / 19600) loss: 1.397344\n",
      "(Iteration 4721 / 19600) loss: 1.541059\n",
      "(Iteration 4741 / 19600) loss: 1.029405\n",
      "(Iteration 4761 / 19600) loss: 1.241815\n",
      "(Iteration 4781 / 19600) loss: 1.140078\n",
      "(Iteration 4801 / 19600) loss: 1.217469\n",
      "(Iteration 4821 / 19600) loss: 1.051722\n",
      "(Iteration 4841 / 19600) loss: 0.934413\n",
      "(Iteration 4861 / 19600) loss: 1.276115\n",
      "(Iteration 4881 / 19600) loss: 1.275148\n",
      "(Epoch 5 / 20) train acc: 0.673000; val_acc: 0.572000\n",
      "(Iteration 4901 / 19600) loss: 1.013051\n",
      "(Iteration 4921 / 19600) loss: 1.309260\n",
      "(Iteration 4941 / 19600) loss: 0.950846\n",
      "(Iteration 4961 / 19600) loss: 1.021931\n",
      "(Iteration 4981 / 19600) loss: 1.004816\n",
      "(Iteration 5001 / 19600) loss: 1.351688\n",
      "(Iteration 5021 / 19600) loss: 1.478042\n",
      "(Iteration 5041 / 19600) loss: 0.941576\n",
      "(Iteration 5061 / 19600) loss: 1.091285\n",
      "(Iteration 5081 / 19600) loss: 1.091023\n",
      "(Iteration 5101 / 19600) loss: 1.002263\n",
      "(Iteration 5121 / 19600) loss: 0.986342\n",
      "(Iteration 5141 / 19600) loss: 1.084403\n",
      "(Iteration 5161 / 19600) loss: 1.269121\n",
      "(Iteration 5181 / 19600) loss: 1.380679\n",
      "(Iteration 5201 / 19600) loss: 1.180233\n",
      "(Iteration 5221 / 19600) loss: 1.165659\n",
      "(Iteration 5241 / 19600) loss: 1.212070\n",
      "(Iteration 5261 / 19600) loss: 1.383327\n",
      "(Iteration 5281 / 19600) loss: 1.248381\n",
      "(Iteration 5301 / 19600) loss: 1.036274\n",
      "(Iteration 5321 / 19600) loss: 0.883584\n",
      "(Iteration 5341 / 19600) loss: 1.243102\n",
      "(Iteration 5361 / 19600) loss: 1.105408\n",
      "(Iteration 5381 / 19600) loss: 0.910985\n",
      "(Iteration 5401 / 19600) loss: 1.001223\n",
      "(Iteration 5421 / 19600) loss: 1.233731\n",
      "(Iteration 5441 / 19600) loss: 1.000169\n",
      "(Iteration 5461 / 19600) loss: 1.120876\n",
      "(Iteration 5481 / 19600) loss: 1.021181\n",
      "(Iteration 5501 / 19600) loss: 1.162982\n",
      "(Iteration 5521 / 19600) loss: 1.016839\n",
      "(Iteration 5541 / 19600) loss: 1.446998\n",
      "(Iteration 5561 / 19600) loss: 1.319625\n",
      "(Iteration 5581 / 19600) loss: 1.389762\n",
      "(Iteration 5601 / 19600) loss: 1.192353\n",
      "(Iteration 5621 / 19600) loss: 1.104057\n",
      "(Iteration 5641 / 19600) loss: 1.013223\n",
      "(Iteration 5661 / 19600) loss: 1.296653\n",
      "(Iteration 5681 / 19600) loss: 1.221388\n",
      "(Iteration 5701 / 19600) loss: 1.481088\n",
      "(Iteration 5721 / 19600) loss: 1.106241\n",
      "(Iteration 5741 / 19600) loss: 1.121374\n",
      "(Iteration 5761 / 19600) loss: 1.042232\n",
      "(Iteration 5781 / 19600) loss: 1.195297\n",
      "(Iteration 5801 / 19600) loss: 1.242290\n",
      "(Iteration 5821 / 19600) loss: 1.236327\n",
      "(Iteration 5841 / 19600) loss: 1.128245\n",
      "(Iteration 5861 / 19600) loss: 1.027327\n",
      "(Epoch 6 / 20) train acc: 0.658000; val_acc: 0.558000\n",
      "(Iteration 5881 / 19600) loss: 1.126946\n",
      "(Iteration 5901 / 19600) loss: 1.546940\n",
      "(Iteration 5921 / 19600) loss: 1.197142\n",
      "(Iteration 5941 / 19600) loss: 1.084522\n",
      "(Iteration 5961 / 19600) loss: 0.986797\n",
      "(Iteration 5981 / 19600) loss: 1.280319\n",
      "(Iteration 6001 / 19600) loss: 1.600026\n",
      "(Iteration 6021 / 19600) loss: 1.060207\n",
      "(Iteration 6041 / 19600) loss: 1.134130\n",
      "(Iteration 6061 / 19600) loss: 1.053091\n",
      "(Iteration 6081 / 19600) loss: 0.977603\n",
      "(Iteration 6101 / 19600) loss: 0.956714\n",
      "(Iteration 6121 / 19600) loss: 1.309769\n",
      "(Iteration 6141 / 19600) loss: 1.049769\n",
      "(Iteration 6161 / 19600) loss: 1.464789\n",
      "(Iteration 6181 / 19600) loss: 1.125881\n",
      "(Iteration 6201 / 19600) loss: 1.290015\n",
      "(Iteration 6221 / 19600) loss: 1.061126\n",
      "(Iteration 6241 / 19600) loss: 1.156880\n",
      "(Iteration 6261 / 19600) loss: 1.260045\n",
      "(Iteration 6281 / 19600) loss: 1.139099\n",
      "(Iteration 6301 / 19600) loss: 1.056718\n",
      "(Iteration 6321 / 19600) loss: 1.263790\n",
      "(Iteration 6341 / 19600) loss: 1.075321\n",
      "(Iteration 6361 / 19600) loss: 1.052855\n",
      "(Iteration 6381 / 19600) loss: 1.122631\n",
      "(Iteration 6401 / 19600) loss: 1.269974\n",
      "(Iteration 6421 / 19600) loss: 1.579645\n",
      "(Iteration 6441 / 19600) loss: 1.209060\n",
      "(Iteration 6461 / 19600) loss: 1.140909\n",
      "(Iteration 6481 / 19600) loss: 1.258105\n",
      "(Iteration 6501 / 19600) loss: 0.938271\n",
      "(Iteration 6521 / 19600) loss: 1.119316\n",
      "(Iteration 6541 / 19600) loss: 0.919857\n",
      "(Iteration 6561 / 19600) loss: 1.457482\n",
      "(Iteration 6581 / 19600) loss: 1.156870\n",
      "(Iteration 6601 / 19600) loss: 0.949794\n",
      "(Iteration 6621 / 19600) loss: 1.356484\n",
      "(Iteration 6641 / 19600) loss: 1.098279\n",
      "(Iteration 6661 / 19600) loss: 1.154590\n",
      "(Iteration 6681 / 19600) loss: 1.377941\n",
      "(Iteration 6701 / 19600) loss: 1.017294\n",
      "(Iteration 6721 / 19600) loss: 1.281714\n",
      "(Iteration 6741 / 19600) loss: 1.294135\n",
      "(Iteration 6761 / 19600) loss: 0.937621\n",
      "(Iteration 6781 / 19600) loss: 1.114018\n",
      "(Iteration 6801 / 19600) loss: 1.036333\n",
      "(Iteration 6821 / 19600) loss: 1.099418\n",
      "(Iteration 6841 / 19600) loss: 1.207537\n",
      "(Epoch 7 / 20) train acc: 0.710000; val_acc: 0.589000\n",
      "(Iteration 6861 / 19600) loss: 1.079043\n",
      "(Iteration 6881 / 19600) loss: 1.032243\n",
      "(Iteration 6901 / 19600) loss: 1.234411\n",
      "(Iteration 6921 / 19600) loss: 1.114607\n",
      "(Iteration 6941 / 19600) loss: 0.962828\n",
      "(Iteration 6961 / 19600) loss: 1.210019\n",
      "(Iteration 6981 / 19600) loss: 1.095518\n",
      "(Iteration 7001 / 19600) loss: 1.141166\n",
      "(Iteration 7021 / 19600) loss: 1.310210\n",
      "(Iteration 7041 / 19600) loss: 1.198058\n",
      "(Iteration 7061 / 19600) loss: 1.057657\n",
      "(Iteration 7081 / 19600) loss: 1.005823\n",
      "(Iteration 7101 / 19600) loss: 0.875238\n",
      "(Iteration 7121 / 19600) loss: 0.968901\n",
      "(Iteration 7141 / 19600) loss: 0.989148\n",
      "(Iteration 7161 / 19600) loss: 1.214272\n",
      "(Iteration 7181 / 19600) loss: 1.030058\n",
      "(Iteration 7201 / 19600) loss: 0.976218\n",
      "(Iteration 7221 / 19600) loss: 0.921393\n",
      "(Iteration 7241 / 19600) loss: 0.967964\n",
      "(Iteration 7261 / 19600) loss: 1.099508\n",
      "(Iteration 7281 / 19600) loss: 1.212632\n",
      "(Iteration 7301 / 19600) loss: 0.892716\n",
      "(Iteration 7321 / 19600) loss: 1.015359\n",
      "(Iteration 7341 / 19600) loss: 0.980111\n",
      "(Iteration 7361 / 19600) loss: 1.119133\n",
      "(Iteration 7381 / 19600) loss: 1.441135\n",
      "(Iteration 7401 / 19600) loss: 1.085743\n",
      "(Iteration 7421 / 19600) loss: 1.377808\n",
      "(Iteration 7441 / 19600) loss: 1.288603\n",
      "(Iteration 7461 / 19600) loss: 1.327807\n",
      "(Iteration 7481 / 19600) loss: 1.269635\n",
      "(Iteration 7501 / 19600) loss: 0.938245\n",
      "(Iteration 7521 / 19600) loss: 1.202811\n",
      "(Iteration 7541 / 19600) loss: 1.098821\n",
      "(Iteration 7561 / 19600) loss: 1.145729\n",
      "(Iteration 7581 / 19600) loss: 1.128504\n",
      "(Iteration 7601 / 19600) loss: 0.983340\n",
      "(Iteration 7621 / 19600) loss: 1.311400\n",
      "(Iteration 7641 / 19600) loss: 1.051727\n",
      "(Iteration 7661 / 19600) loss: 0.764764\n",
      "(Iteration 7681 / 19600) loss: 1.056121\n",
      "(Iteration 7701 / 19600) loss: 1.043603\n",
      "(Iteration 7721 / 19600) loss: 1.431750\n",
      "(Iteration 7741 / 19600) loss: 1.116507\n",
      "(Iteration 7761 / 19600) loss: 0.958812\n",
      "(Iteration 7781 / 19600) loss: 1.584638\n",
      "(Iteration 7801 / 19600) loss: 1.393975\n",
      "(Iteration 7821 / 19600) loss: 1.074628\n",
      "(Epoch 8 / 20) train acc: 0.745000; val_acc: 0.603000\n",
      "(Iteration 7841 / 19600) loss: 1.303426\n",
      "(Iteration 7861 / 19600) loss: 1.228721\n",
      "(Iteration 7881 / 19600) loss: 1.217317\n",
      "(Iteration 7901 / 19600) loss: 1.237504\n",
      "(Iteration 7921 / 19600) loss: 1.197911\n",
      "(Iteration 7941 / 19600) loss: 0.945308\n",
      "(Iteration 7961 / 19600) loss: 1.208192\n",
      "(Iteration 7981 / 19600) loss: 1.047580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 8001 / 19600) loss: 1.029349\n",
      "(Iteration 8021 / 19600) loss: 1.111033\n",
      "(Iteration 8041 / 19600) loss: 1.250560\n",
      "(Iteration 8061 / 19600) loss: 1.046013\n",
      "(Iteration 8081 / 19600) loss: 1.262072\n",
      "(Iteration 8101 / 19600) loss: 1.242891\n",
      "(Iteration 8121 / 19600) loss: 1.102445\n",
      "(Iteration 8141 / 19600) loss: 1.304506\n",
      "(Iteration 8161 / 19600) loss: 1.454001\n",
      "(Iteration 8181 / 19600) loss: 1.057807\n",
      "(Iteration 8201 / 19600) loss: 1.200251\n",
      "(Iteration 8221 / 19600) loss: 0.794245\n",
      "(Iteration 8241 / 19600) loss: 0.891624\n",
      "(Iteration 8261 / 19600) loss: 0.859519\n",
      "(Iteration 8281 / 19600) loss: 0.915602\n",
      "(Iteration 8301 / 19600) loss: 0.919624\n",
      "(Iteration 8321 / 19600) loss: 1.089198\n",
      "(Iteration 8341 / 19600) loss: 1.297584\n",
      "(Iteration 8361 / 19600) loss: 1.281805\n",
      "(Iteration 8381 / 19600) loss: 1.164052\n",
      "(Iteration 8401 / 19600) loss: 1.312942\n",
      "(Iteration 8421 / 19600) loss: 0.949726\n",
      "(Iteration 8441 / 19600) loss: 1.195015\n",
      "(Iteration 8461 / 19600) loss: 1.022221\n",
      "(Iteration 8481 / 19600) loss: 0.998762\n",
      "(Iteration 8501 / 19600) loss: 1.353475\n",
      "(Iteration 8521 / 19600) loss: 1.130154\n",
      "(Iteration 8541 / 19600) loss: 1.427139\n",
      "(Iteration 8561 / 19600) loss: 0.811163\n",
      "(Iteration 8581 / 19600) loss: 1.383017\n",
      "(Iteration 8601 / 19600) loss: 1.008701\n",
      "(Iteration 8621 / 19600) loss: 1.035759\n",
      "(Iteration 8641 / 19600) loss: 0.916734\n",
      "(Iteration 8661 / 19600) loss: 1.166333\n",
      "(Iteration 8681 / 19600) loss: 1.156711\n",
      "(Iteration 8701 / 19600) loss: 1.048263\n",
      "(Iteration 8721 / 19600) loss: 1.322667\n",
      "(Iteration 8741 / 19600) loss: 1.095172\n",
      "(Iteration 8761 / 19600) loss: 1.122648\n",
      "(Iteration 8781 / 19600) loss: 1.108053\n",
      "(Iteration 8801 / 19600) loss: 1.027672\n",
      "(Epoch 9 / 20) train acc: 0.723000; val_acc: 0.586000\n",
      "(Iteration 8821 / 19600) loss: 1.185089\n",
      "(Iteration 8841 / 19600) loss: 0.970698\n",
      "(Iteration 8861 / 19600) loss: 0.951042\n",
      "(Iteration 8881 / 19600) loss: 1.065186\n",
      "(Iteration 8901 / 19600) loss: 1.188926\n",
      "(Iteration 8921 / 19600) loss: 1.030602\n",
      "(Iteration 8941 / 19600) loss: 1.296132\n",
      "(Iteration 8961 / 19600) loss: 1.227763\n",
      "(Iteration 8981 / 19600) loss: 0.836180\n",
      "(Iteration 9001 / 19600) loss: 1.160660\n",
      "(Iteration 9021 / 19600) loss: 1.193383\n",
      "(Iteration 9041 / 19600) loss: 1.219647\n",
      "(Iteration 9061 / 19600) loss: 1.078744\n",
      "(Iteration 9081 / 19600) loss: 1.039206\n",
      "(Iteration 9101 / 19600) loss: 1.437039\n",
      "(Iteration 9121 / 19600) loss: 1.423971\n",
      "(Iteration 9141 / 19600) loss: 1.015839\n",
      "(Iteration 9161 / 19600) loss: 1.076789\n",
      "(Iteration 9181 / 19600) loss: 1.103151\n",
      "(Iteration 9201 / 19600) loss: 0.994074\n",
      "(Iteration 9221 / 19600) loss: 0.983830\n",
      "(Iteration 9241 / 19600) loss: 0.985682\n",
      "(Iteration 9261 / 19600) loss: 1.440140\n",
      "(Iteration 9281 / 19600) loss: 1.126791\n",
      "(Iteration 9301 / 19600) loss: 0.932868\n",
      "(Iteration 9321 / 19600) loss: 0.926973\n",
      "(Iteration 9341 / 19600) loss: 0.889040\n",
      "(Iteration 9361 / 19600) loss: 0.971840\n",
      "(Iteration 9381 / 19600) loss: 1.155135\n",
      "(Iteration 9401 / 19600) loss: 1.141925\n",
      "(Iteration 9421 / 19600) loss: 1.166825\n",
      "(Iteration 9441 / 19600) loss: 1.034207\n",
      "(Iteration 9461 / 19600) loss: 0.876456\n",
      "(Iteration 9481 / 19600) loss: 1.179201\n",
      "(Iteration 9501 / 19600) loss: 0.897447\n",
      "(Iteration 9521 / 19600) loss: 1.042161\n",
      "(Iteration 9541 / 19600) loss: 1.047424\n",
      "(Iteration 9561 / 19600) loss: 1.094634\n",
      "(Iteration 9581 / 19600) loss: 1.094600\n",
      "(Iteration 9601 / 19600) loss: 0.903356\n",
      "(Iteration 9621 / 19600) loss: 1.004895\n",
      "(Iteration 9641 / 19600) loss: 1.048725\n",
      "(Iteration 9661 / 19600) loss: 1.124764\n",
      "(Iteration 9681 / 19600) loss: 0.970528\n",
      "(Iteration 9701 / 19600) loss: 0.795319\n",
      "(Iteration 9721 / 19600) loss: 1.186013\n",
      "(Iteration 9741 / 19600) loss: 0.981233\n",
      "(Iteration 9761 / 19600) loss: 0.889898\n",
      "(Iteration 9781 / 19600) loss: 1.018711\n",
      "(Epoch 10 / 20) train acc: 0.765000; val_acc: 0.602000\n",
      "(Iteration 9801 / 19600) loss: 1.265799\n",
      "(Iteration 9821 / 19600) loss: 0.932829\n",
      "(Iteration 9841 / 19600) loss: 0.895221\n",
      "(Iteration 9861 / 19600) loss: 0.929144\n",
      "(Iteration 9881 / 19600) loss: 1.049140\n",
      "(Iteration 9901 / 19600) loss: 0.985213\n",
      "(Iteration 9921 / 19600) loss: 1.226191\n",
      "(Iteration 9941 / 19600) loss: 0.913159\n",
      "(Iteration 9961 / 19600) loss: 1.090065\n",
      "(Iteration 9981 / 19600) loss: 0.950316\n",
      "(Iteration 10001 / 19600) loss: 1.258110\n",
      "(Iteration 10021 / 19600) loss: 1.139861\n",
      "(Iteration 10041 / 19600) loss: 0.867995\n",
      "(Iteration 10061 / 19600) loss: 1.160976\n",
      "(Iteration 10081 / 19600) loss: 0.824066\n",
      "(Iteration 10101 / 19600) loss: 1.056418\n",
      "(Iteration 10121 / 19600) loss: 0.798888\n",
      "(Iteration 10141 / 19600) loss: 1.250578\n",
      "(Iteration 10161 / 19600) loss: 0.957240\n",
      "(Iteration 10181 / 19600) loss: 1.015491\n",
      "(Iteration 10201 / 19600) loss: 1.012848\n",
      "(Iteration 10221 / 19600) loss: 1.070618\n",
      "(Iteration 10241 / 19600) loss: 0.984141\n",
      "(Iteration 10261 / 19600) loss: 1.023008\n",
      "(Iteration 10281 / 19600) loss: 1.152210\n",
      "(Iteration 10301 / 19600) loss: 1.147123\n",
      "(Iteration 10321 / 19600) loss: 1.183310\n",
      "(Iteration 10341 / 19600) loss: 1.028021\n",
      "(Iteration 10361 / 19600) loss: 1.250141\n",
      "(Iteration 10381 / 19600) loss: 0.974472\n",
      "(Iteration 10401 / 19600) loss: 1.197133\n",
      "(Iteration 10421 / 19600) loss: 0.885914\n",
      "(Iteration 10441 / 19600) loss: 1.289684\n",
      "(Iteration 10461 / 19600) loss: 0.919606\n",
      "(Iteration 10481 / 19600) loss: 0.920457\n",
      "(Iteration 10501 / 19600) loss: 1.129111\n",
      "(Iteration 10521 / 19600) loss: 0.953793\n",
      "(Iteration 10541 / 19600) loss: 1.158095\n",
      "(Iteration 10561 / 19600) loss: 1.083952\n",
      "(Iteration 10581 / 19600) loss: 1.067492\n",
      "(Iteration 10601 / 19600) loss: 1.047083\n",
      "(Iteration 10621 / 19600) loss: 1.284570\n",
      "(Iteration 10641 / 19600) loss: 0.872762\n",
      "(Iteration 10661 / 19600) loss: 1.002230\n",
      "(Iteration 10681 / 19600) loss: 1.147541\n",
      "(Iteration 10701 / 19600) loss: 0.914020\n",
      "(Iteration 10721 / 19600) loss: 1.024262\n",
      "(Iteration 10741 / 19600) loss: 1.194392\n",
      "(Iteration 10761 / 19600) loss: 1.210819\n",
      "(Epoch 11 / 20) train acc: 0.743000; val_acc: 0.614000\n",
      "(Iteration 10781 / 19600) loss: 1.031760\n",
      "(Iteration 10801 / 19600) loss: 1.049718\n",
      "(Iteration 10821 / 19600) loss: 1.060771\n",
      "(Iteration 10841 / 19600) loss: 1.036009\n",
      "(Iteration 10861 / 19600) loss: 0.815403\n",
      "(Iteration 10881 / 19600) loss: 1.015007\n",
      "(Iteration 10901 / 19600) loss: 0.905174\n",
      "(Iteration 10921 / 19600) loss: 1.093952\n",
      "(Iteration 10941 / 19600) loss: 0.915090\n",
      "(Iteration 10961 / 19600) loss: 1.432668\n",
      "(Iteration 10981 / 19600) loss: 1.009196\n",
      "(Iteration 11001 / 19600) loss: 1.253239\n",
      "(Iteration 11021 / 19600) loss: 1.168057\n",
      "(Iteration 11041 / 19600) loss: 1.174125\n",
      "(Iteration 11061 / 19600) loss: 1.024692\n",
      "(Iteration 11081 / 19600) loss: 1.136546\n",
      "(Iteration 11101 / 19600) loss: 0.982874\n",
      "(Iteration 11121 / 19600) loss: 1.243306\n",
      "(Iteration 11141 / 19600) loss: 1.211845\n",
      "(Iteration 11161 / 19600) loss: 1.023982\n",
      "(Iteration 11181 / 19600) loss: 1.011218\n",
      "(Iteration 11201 / 19600) loss: 1.235629\n",
      "(Iteration 11221 / 19600) loss: 1.176461\n",
      "(Iteration 11241 / 19600) loss: 1.195875\n",
      "(Iteration 11261 / 19600) loss: 0.798345\n",
      "(Iteration 11281 / 19600) loss: 1.303260\n",
      "(Iteration 11301 / 19600) loss: 0.967119\n",
      "(Iteration 11321 / 19600) loss: 1.325418\n",
      "(Iteration 11341 / 19600) loss: 1.054896\n",
      "(Iteration 11361 / 19600) loss: 1.053064\n",
      "(Iteration 11381 / 19600) loss: 1.126773\n",
      "(Iteration 11401 / 19600) loss: 1.319713\n",
      "(Iteration 11421 / 19600) loss: 1.138100\n",
      "(Iteration 11441 / 19600) loss: 0.858862\n",
      "(Iteration 11461 / 19600) loss: 1.330801\n",
      "(Iteration 11481 / 19600) loss: 0.948324\n",
      "(Iteration 11501 / 19600) loss: 0.900838\n",
      "(Iteration 11521 / 19600) loss: 0.883015\n",
      "(Iteration 11541 / 19600) loss: 1.039934\n",
      "(Iteration 11561 / 19600) loss: 0.951247\n",
      "(Iteration 11581 / 19600) loss: 1.209360\n",
      "(Iteration 11601 / 19600) loss: 1.027974\n",
      "(Iteration 11621 / 19600) loss: 1.138161\n",
      "(Iteration 11641 / 19600) loss: 1.110977\n",
      "(Iteration 11661 / 19600) loss: 0.922771\n",
      "(Iteration 11681 / 19600) loss: 1.222405\n",
      "(Iteration 11701 / 19600) loss: 1.285269\n",
      "(Iteration 11721 / 19600) loss: 0.912114\n",
      "(Iteration 11741 / 19600) loss: 0.913789\n",
      "(Epoch 12 / 20) train acc: 0.743000; val_acc: 0.614000\n",
      "(Iteration 11761 / 19600) loss: 1.275592\n",
      "(Iteration 11781 / 19600) loss: 1.004045\n",
      "(Iteration 11801 / 19600) loss: 0.994785\n",
      "(Iteration 11821 / 19600) loss: 1.078238\n",
      "(Iteration 11841 / 19600) loss: 1.243113\n",
      "(Iteration 11861 / 19600) loss: 1.185626\n",
      "(Iteration 11881 / 19600) loss: 0.961423\n",
      "(Iteration 11901 / 19600) loss: 1.166969\n",
      "(Iteration 11921 / 19600) loss: 0.986938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 11941 / 19600) loss: 1.185182\n",
      "(Iteration 11961 / 19600) loss: 1.235863\n",
      "(Iteration 11981 / 19600) loss: 0.982002\n",
      "(Iteration 12001 / 19600) loss: 1.184722\n",
      "(Iteration 12021 / 19600) loss: 1.227906\n",
      "(Iteration 12041 / 19600) loss: 1.259490\n",
      "(Iteration 12061 / 19600) loss: 1.190093\n",
      "(Iteration 12081 / 19600) loss: 1.026900\n",
      "(Iteration 12101 / 19600) loss: 1.157349\n",
      "(Iteration 12121 / 19600) loss: 0.979099\n",
      "(Iteration 12141 / 19600) loss: 1.076361\n",
      "(Iteration 12161 / 19600) loss: 1.336380\n",
      "(Iteration 12181 / 19600) loss: 1.300714\n",
      "(Iteration 12201 / 19600) loss: 1.058791\n",
      "(Iteration 12221 / 19600) loss: 1.067798\n",
      "(Iteration 12241 / 19600) loss: 1.001786\n",
      "(Iteration 12261 / 19600) loss: 1.069544\n",
      "(Iteration 12281 / 19600) loss: 1.418471\n",
      "(Iteration 12301 / 19600) loss: 1.167618\n",
      "(Iteration 12321 / 19600) loss: 1.003516\n",
      "(Iteration 12341 / 19600) loss: 1.193664\n",
      "(Iteration 12361 / 19600) loss: 1.097350\n",
      "(Iteration 12381 / 19600) loss: 0.951055\n",
      "(Iteration 12401 / 19600) loss: 0.980827\n",
      "(Iteration 12421 / 19600) loss: 0.836644\n",
      "(Iteration 12441 / 19600) loss: 0.923356\n",
      "(Iteration 12461 / 19600) loss: 0.935899\n",
      "(Iteration 12481 / 19600) loss: 1.504594\n",
      "(Iteration 12501 / 19600) loss: 1.131175\n",
      "(Iteration 12521 / 19600) loss: 1.208558\n",
      "(Iteration 12541 / 19600) loss: 0.993198\n",
      "(Iteration 12561 / 19600) loss: 0.803593\n",
      "(Iteration 12581 / 19600) loss: 1.081265\n",
      "(Iteration 12601 / 19600) loss: 0.860706\n",
      "(Iteration 12621 / 19600) loss: 1.229206\n",
      "(Iteration 12641 / 19600) loss: 0.917451\n",
      "(Iteration 12661 / 19600) loss: 0.834042\n",
      "(Iteration 12681 / 19600) loss: 1.161880\n",
      "(Iteration 12701 / 19600) loss: 1.187043\n",
      "(Iteration 12721 / 19600) loss: 0.931674\n",
      "(Epoch 13 / 20) train acc: 0.764000; val_acc: 0.594000\n",
      "(Iteration 12741 / 19600) loss: 1.008241\n",
      "(Iteration 12761 / 19600) loss: 1.073620\n",
      "(Iteration 12781 / 19600) loss: 0.922038\n",
      "(Iteration 12801 / 19600) loss: 1.021249\n",
      "(Iteration 12821 / 19600) loss: 1.256933\n",
      "(Iteration 12841 / 19600) loss: 1.343944\n",
      "(Iteration 12861 / 19600) loss: 1.272015\n",
      "(Iteration 12881 / 19600) loss: 1.021926\n",
      "(Iteration 12901 / 19600) loss: 1.290674\n",
      "(Iteration 12921 / 19600) loss: 1.035649\n",
      "(Iteration 12941 / 19600) loss: 0.946265\n",
      "(Iteration 12961 / 19600) loss: 1.204288\n",
      "(Iteration 12981 / 19600) loss: 1.120493\n",
      "(Iteration 13001 / 19600) loss: 0.954693\n",
      "(Iteration 13021 / 19600) loss: 0.887557\n",
      "(Iteration 13041 / 19600) loss: 1.335907\n",
      "(Iteration 13061 / 19600) loss: 1.075007\n",
      "(Iteration 13081 / 19600) loss: 1.086521\n",
      "(Iteration 13101 / 19600) loss: 1.327688\n",
      "(Iteration 13121 / 19600) loss: 1.246560\n",
      "(Iteration 13141 / 19600) loss: 0.886194\n",
      "(Iteration 13161 / 19600) loss: 0.997825\n",
      "(Iteration 13181 / 19600) loss: 1.128322\n",
      "(Iteration 13201 / 19600) loss: 0.961992\n",
      "(Iteration 13221 / 19600) loss: 0.921864\n",
      "(Iteration 13241 / 19600) loss: 1.119973\n",
      "(Iteration 13261 / 19600) loss: 0.982614\n",
      "(Iteration 13281 / 19600) loss: 0.978283\n",
      "(Iteration 13301 / 19600) loss: 0.962482\n",
      "(Iteration 13321 / 19600) loss: 1.030520\n",
      "(Iteration 13341 / 19600) loss: 0.941244\n",
      "(Iteration 13361 / 19600) loss: 1.304115\n",
      "(Iteration 13381 / 19600) loss: 1.027703\n",
      "(Iteration 13401 / 19600) loss: 1.110942\n",
      "(Iteration 13421 / 19600) loss: 1.062392\n",
      "(Iteration 13441 / 19600) loss: 1.366799\n",
      "(Iteration 13461 / 19600) loss: 1.028731\n",
      "(Iteration 13481 / 19600) loss: 0.922742\n",
      "(Iteration 13501 / 19600) loss: 1.091937\n",
      "(Iteration 13521 / 19600) loss: 1.190539\n",
      "(Iteration 13541 / 19600) loss: 1.060846\n",
      "(Iteration 13561 / 19600) loss: 1.346247\n",
      "(Iteration 13581 / 19600) loss: 1.070466\n",
      "(Iteration 13601 / 19600) loss: 0.857015\n",
      "(Iteration 13621 / 19600) loss: 1.044248\n",
      "(Iteration 13641 / 19600) loss: 1.068240\n",
      "(Iteration 13661 / 19600) loss: 1.090951\n",
      "(Iteration 13681 / 19600) loss: 1.009293\n",
      "(Iteration 13701 / 19600) loss: 1.150168\n",
      "(Epoch 14 / 20) train acc: 0.735000; val_acc: 0.597000\n",
      "(Iteration 13721 / 19600) loss: 1.001232\n",
      "(Iteration 13741 / 19600) loss: 1.207411\n",
      "(Iteration 13761 / 19600) loss: 1.182178\n",
      "(Iteration 13781 / 19600) loss: 1.185895\n",
      "(Iteration 13801 / 19600) loss: 1.150692\n",
      "(Iteration 13821 / 19600) loss: 1.160939\n",
      "(Iteration 13841 / 19600) loss: 0.910177\n",
      "(Iteration 13861 / 19600) loss: 0.975708\n",
      "(Iteration 13881 / 19600) loss: 0.765674\n",
      "(Iteration 13901 / 19600) loss: 1.169250\n",
      "(Iteration 13921 / 19600) loss: 1.271045\n",
      "(Iteration 13941 / 19600) loss: 1.247431\n",
      "(Iteration 13961 / 19600) loss: 0.978387\n",
      "(Iteration 13981 / 19600) loss: 1.064206\n",
      "(Iteration 14001 / 19600) loss: 0.926958\n",
      "(Iteration 14021 / 19600) loss: 1.000684\n",
      "(Iteration 14041 / 19600) loss: 1.167376\n",
      "(Iteration 14061 / 19600) loss: 0.923388\n",
      "(Iteration 14081 / 19600) loss: 1.147747\n",
      "(Iteration 14101 / 19600) loss: 1.227016\n",
      "(Iteration 14121 / 19600) loss: 0.881638\n",
      "(Iteration 14141 / 19600) loss: 1.427221\n",
      "(Iteration 14161 / 19600) loss: 1.158294\n",
      "(Iteration 14181 / 19600) loss: 1.271486\n",
      "(Iteration 14201 / 19600) loss: 1.001273\n",
      "(Iteration 14221 / 19600) loss: 1.453621\n",
      "(Iteration 14241 / 19600) loss: 1.133052\n",
      "(Iteration 14261 / 19600) loss: 1.166276\n",
      "(Iteration 14281 / 19600) loss: 1.272658\n",
      "(Iteration 14301 / 19600) loss: 0.833368\n",
      "(Iteration 14321 / 19600) loss: 1.232925\n",
      "(Iteration 14341 / 19600) loss: 1.248655\n",
      "(Iteration 14361 / 19600) loss: 1.233711\n",
      "(Iteration 14381 / 19600) loss: 0.899260\n",
      "(Iteration 14401 / 19600) loss: 1.061674\n",
      "(Iteration 14421 / 19600) loss: 1.069630\n",
      "(Iteration 14441 / 19600) loss: 0.785976\n",
      "(Iteration 14461 / 19600) loss: 1.204614\n",
      "(Iteration 14481 / 19600) loss: 1.072992\n",
      "(Iteration 14501 / 19600) loss: 1.310940\n",
      "(Iteration 14521 / 19600) loss: 0.970214\n",
      "(Iteration 14541 / 19600) loss: 0.956441\n",
      "(Iteration 14561 / 19600) loss: 0.883476\n",
      "(Iteration 14581 / 19600) loss: 0.996089\n",
      "(Iteration 14601 / 19600) loss: 1.115068\n",
      "(Iteration 14621 / 19600) loss: 0.788707\n",
      "(Iteration 14641 / 19600) loss: 0.874783\n",
      "(Iteration 14661 / 19600) loss: 0.926956\n",
      "(Iteration 14681 / 19600) loss: 1.214572\n",
      "(Epoch 15 / 20) train acc: 0.757000; val_acc: 0.613000\n",
      "(Iteration 14701 / 19600) loss: 1.173287\n",
      "(Iteration 14721 / 19600) loss: 1.016431\n",
      "(Iteration 14741 / 19600) loss: 1.034594\n",
      "(Iteration 14761 / 19600) loss: 0.868039\n",
      "(Iteration 14781 / 19600) loss: 1.006163\n",
      "(Iteration 14801 / 19600) loss: 1.184258\n",
      "(Iteration 14821 / 19600) loss: 1.190585\n",
      "(Iteration 14841 / 19600) loss: 1.075639\n",
      "(Iteration 14861 / 19600) loss: 1.245800\n",
      "(Iteration 14881 / 19600) loss: 1.082746\n",
      "(Iteration 14901 / 19600) loss: 1.099511\n",
      "(Iteration 14921 / 19600) loss: 0.994888\n",
      "(Iteration 14941 / 19600) loss: 0.999578\n",
      "(Iteration 14961 / 19600) loss: 0.764798\n",
      "(Iteration 14981 / 19600) loss: 1.236363\n",
      "(Iteration 15001 / 19600) loss: 1.202060\n",
      "(Iteration 15021 / 19600) loss: 1.226053\n",
      "(Iteration 15041 / 19600) loss: 0.813403\n",
      "(Iteration 15061 / 19600) loss: 1.083527\n",
      "(Iteration 15081 / 19600) loss: 1.052202\n",
      "(Iteration 15101 / 19600) loss: 1.184969\n",
      "(Iteration 15121 / 19600) loss: 1.376521\n",
      "(Iteration 15141 / 19600) loss: 0.980112\n",
      "(Iteration 15161 / 19600) loss: 0.890107\n",
      "(Iteration 15181 / 19600) loss: 1.002233\n",
      "(Iteration 15201 / 19600) loss: 1.258027\n",
      "(Iteration 15221 / 19600) loss: 0.921090\n",
      "(Iteration 15241 / 19600) loss: 1.150503\n",
      "(Iteration 15261 / 19600) loss: 0.809558\n",
      "(Iteration 15281 / 19600) loss: 0.973472\n",
      "(Iteration 15301 / 19600) loss: 0.914020\n",
      "(Iteration 15321 / 19600) loss: 1.100673\n",
      "(Iteration 15341 / 19600) loss: 1.076039\n",
      "(Iteration 15361 / 19600) loss: 0.939697\n",
      "(Iteration 15381 / 19600) loss: 1.053708\n",
      "(Iteration 15401 / 19600) loss: 1.085685\n",
      "(Iteration 15421 / 19600) loss: 0.940142\n",
      "(Iteration 15441 / 19600) loss: 0.917016\n",
      "(Iteration 15461 / 19600) loss: 1.139950\n",
      "(Iteration 15481 / 19600) loss: 1.001313\n",
      "(Iteration 15501 / 19600) loss: 0.927881\n",
      "(Iteration 15521 / 19600) loss: 0.981929\n",
      "(Iteration 15541 / 19600) loss: 1.064296\n",
      "(Iteration 15561 / 19600) loss: 1.016904\n",
      "(Iteration 15581 / 19600) loss: 1.399627\n",
      "(Iteration 15601 / 19600) loss: 1.113409\n",
      "(Iteration 15621 / 19600) loss: 1.134256\n",
      "(Iteration 15641 / 19600) loss: 1.167437\n",
      "(Iteration 15661 / 19600) loss: 1.152645\n",
      "(Epoch 16 / 20) train acc: 0.751000; val_acc: 0.619000\n",
      "(Iteration 15681 / 19600) loss: 0.887625\n",
      "(Iteration 15701 / 19600) loss: 0.856523\n",
      "(Iteration 15721 / 19600) loss: 0.999750\n",
      "(Iteration 15741 / 19600) loss: 0.967461\n",
      "(Iteration 15761 / 19600) loss: 1.194128\n",
      "(Iteration 15781 / 19600) loss: 1.019217\n",
      "(Iteration 15801 / 19600) loss: 1.136332\n",
      "(Iteration 15821 / 19600) loss: 1.290342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 15841 / 19600) loss: 0.934038\n",
      "(Iteration 15861 / 19600) loss: 1.261346\n",
      "(Iteration 15881 / 19600) loss: 1.412228\n",
      "(Iteration 15901 / 19600) loss: 1.165731\n",
      "(Iteration 15921 / 19600) loss: 1.314209\n",
      "(Iteration 15941 / 19600) loss: 1.200224\n",
      "(Iteration 15961 / 19600) loss: 0.969314\n",
      "(Iteration 15981 / 19600) loss: 1.011904\n",
      "(Iteration 16001 / 19600) loss: 1.216191\n",
      "(Iteration 16021 / 19600) loss: 1.313586\n",
      "(Iteration 16041 / 19600) loss: 1.074776\n",
      "(Iteration 16061 / 19600) loss: 0.883329\n",
      "(Iteration 16081 / 19600) loss: 1.013731\n",
      "(Iteration 16101 / 19600) loss: 0.999982\n",
      "(Iteration 16121 / 19600) loss: 1.120859\n",
      "(Iteration 16141 / 19600) loss: 0.961815\n",
      "(Iteration 16161 / 19600) loss: 1.089998\n",
      "(Iteration 16181 / 19600) loss: 0.979010\n",
      "(Iteration 16201 / 19600) loss: 0.954528\n",
      "(Iteration 16221 / 19600) loss: 1.135460\n",
      "(Iteration 16241 / 19600) loss: 0.860378\n",
      "(Iteration 16261 / 19600) loss: 0.860288\n",
      "(Iteration 16281 / 19600) loss: 1.047135\n",
      "(Iteration 16301 / 19600) loss: 0.979751\n",
      "(Iteration 16321 / 19600) loss: 1.218961\n",
      "(Iteration 16341 / 19600) loss: 0.939866\n",
      "(Iteration 16361 / 19600) loss: 1.060763\n",
      "(Iteration 16381 / 19600) loss: 0.796967\n",
      "(Iteration 16401 / 19600) loss: 0.932105\n",
      "(Iteration 16421 / 19600) loss: 0.795719\n",
      "(Iteration 16441 / 19600) loss: 1.051672\n",
      "(Iteration 16461 / 19600) loss: 1.018113\n",
      "(Iteration 16481 / 19600) loss: 1.191132\n",
      "(Iteration 16501 / 19600) loss: 0.985260\n",
      "(Iteration 16521 / 19600) loss: 0.948187\n",
      "(Iteration 16541 / 19600) loss: 1.028626\n",
      "(Iteration 16561 / 19600) loss: 1.064303\n",
      "(Iteration 16581 / 19600) loss: 1.130833\n",
      "(Iteration 16601 / 19600) loss: 0.758048\n",
      "(Iteration 16621 / 19600) loss: 1.222471\n",
      "(Iteration 16641 / 19600) loss: 1.200749\n",
      "(Epoch 17 / 20) train acc: 0.776000; val_acc: 0.627000\n",
      "(Iteration 16661 / 19600) loss: 0.680493\n",
      "(Iteration 16681 / 19600) loss: 1.112059\n",
      "(Iteration 16701 / 19600) loss: 1.024295\n",
      "(Iteration 16721 / 19600) loss: 0.831032\n",
      "(Iteration 16741 / 19600) loss: 1.203702\n",
      "(Iteration 16761 / 19600) loss: 1.293021\n",
      "(Iteration 16781 / 19600) loss: 1.094645\n",
      "(Iteration 16801 / 19600) loss: 0.915802\n",
      "(Iteration 16821 / 19600) loss: 1.214523\n",
      "(Iteration 16841 / 19600) loss: 0.825366\n",
      "(Iteration 16861 / 19600) loss: 1.072567\n",
      "(Iteration 16881 / 19600) loss: 0.966206\n",
      "(Iteration 16901 / 19600) loss: 1.353817\n",
      "(Iteration 16921 / 19600) loss: 1.423474\n",
      "(Iteration 16941 / 19600) loss: 0.852530\n",
      "(Iteration 16961 / 19600) loss: 1.192785\n",
      "(Iteration 16981 / 19600) loss: 0.988815\n",
      "(Iteration 17001 / 19600) loss: 1.020163\n",
      "(Iteration 17021 / 19600) loss: 1.294463\n",
      "(Iteration 17041 / 19600) loss: 1.057347\n",
      "(Iteration 17061 / 19600) loss: 0.984080\n",
      "(Iteration 17081 / 19600) loss: 1.094445\n",
      "(Iteration 17101 / 19600) loss: 0.929991\n",
      "(Iteration 17121 / 19600) loss: 1.005271\n",
      "(Iteration 17141 / 19600) loss: 0.935400\n",
      "(Iteration 17161 / 19600) loss: 1.023671\n",
      "(Iteration 17181 / 19600) loss: 1.122981\n",
      "(Iteration 17201 / 19600) loss: 0.648482\n",
      "(Iteration 17221 / 19600) loss: 0.899338\n",
      "(Iteration 17241 / 19600) loss: 1.214777\n",
      "(Iteration 17261 / 19600) loss: 0.787066\n",
      "(Iteration 17281 / 19600) loss: 0.866308\n",
      "(Iteration 17301 / 19600) loss: 0.921591\n",
      "(Iteration 17321 / 19600) loss: 1.149556\n",
      "(Iteration 17341 / 19600) loss: 1.264009\n",
      "(Iteration 17361 / 19600) loss: 1.086328\n",
      "(Iteration 17381 / 19600) loss: 1.032143\n",
      "(Iteration 17401 / 19600) loss: 1.113885\n",
      "(Iteration 17421 / 19600) loss: 0.886659\n",
      "(Iteration 17441 / 19600) loss: 0.999999\n",
      "(Iteration 17461 / 19600) loss: 1.125861\n",
      "(Iteration 17481 / 19600) loss: 0.834494\n",
      "(Iteration 17501 / 19600) loss: 1.181104\n",
      "(Iteration 17521 / 19600) loss: 0.853607\n",
      "(Iteration 17541 / 19600) loss: 1.016330\n",
      "(Iteration 17561 / 19600) loss: 0.886287\n",
      "(Iteration 17581 / 19600) loss: 1.394502\n",
      "(Iteration 17601 / 19600) loss: 1.000688\n",
      "(Iteration 17621 / 19600) loss: 1.053647\n",
      "(Epoch 18 / 20) train acc: 0.769000; val_acc: 0.603000\n",
      "(Iteration 17641 / 19600) loss: 1.068415\n",
      "(Iteration 17661 / 19600) loss: 0.984695\n",
      "(Iteration 17681 / 19600) loss: 0.893036\n",
      "(Iteration 17701 / 19600) loss: 0.957766\n",
      "(Iteration 17721 / 19600) loss: 1.178594\n",
      "(Iteration 17741 / 19600) loss: 1.118400\n",
      "(Iteration 17761 / 19600) loss: 1.073411\n",
      "(Iteration 17781 / 19600) loss: 1.048543\n",
      "(Iteration 17801 / 19600) loss: 1.364157\n",
      "(Iteration 17821 / 19600) loss: 0.916349\n",
      "(Iteration 17841 / 19600) loss: 1.216784\n",
      "(Iteration 17861 / 19600) loss: 1.131193\n",
      "(Iteration 17881 / 19600) loss: 1.199759\n",
      "(Iteration 17901 / 19600) loss: 1.124253\n",
      "(Iteration 17921 / 19600) loss: 0.803846\n",
      "(Iteration 17941 / 19600) loss: 1.053451\n",
      "(Iteration 17961 / 19600) loss: 1.122259\n",
      "(Iteration 17981 / 19600) loss: 1.325367\n",
      "(Iteration 18001 / 19600) loss: 0.812538\n",
      "(Iteration 18021 / 19600) loss: 1.046477\n",
      "(Iteration 18041 / 19600) loss: 0.928789\n",
      "(Iteration 18061 / 19600) loss: 1.331465\n",
      "(Iteration 18081 / 19600) loss: 1.026241\n",
      "(Iteration 18101 / 19600) loss: 1.212908\n",
      "(Iteration 18121 / 19600) loss: 1.207326\n",
      "(Iteration 18141 / 19600) loss: 0.943334\n",
      "(Iteration 18161 / 19600) loss: 0.980295\n",
      "(Iteration 18181 / 19600) loss: 1.100391\n",
      "(Iteration 18201 / 19600) loss: 0.890786\n",
      "(Iteration 18221 / 19600) loss: 1.108852\n",
      "(Iteration 18241 / 19600) loss: 0.914702\n",
      "(Iteration 18261 / 19600) loss: 0.942464\n",
      "(Iteration 18281 / 19600) loss: 0.802559\n",
      "(Iteration 18301 / 19600) loss: 1.191999\n",
      "(Iteration 18321 / 19600) loss: 0.954368\n",
      "(Iteration 18341 / 19600) loss: 0.993857\n",
      "(Iteration 18361 / 19600) loss: 0.893275\n",
      "(Iteration 18381 / 19600) loss: 1.028193\n",
      "(Iteration 18401 / 19600) loss: 1.157517\n",
      "(Iteration 18421 / 19600) loss: 1.153618\n",
      "(Iteration 18441 / 19600) loss: 0.762807\n",
      "(Iteration 18461 / 19600) loss: 1.163683\n",
      "(Iteration 18481 / 19600) loss: 0.871110\n",
      "(Iteration 18501 / 19600) loss: 1.267001\n",
      "(Iteration 18521 / 19600) loss: 1.224257\n",
      "(Iteration 18541 / 19600) loss: 0.858154\n",
      "(Iteration 18561 / 19600) loss: 1.093014\n",
      "(Iteration 18581 / 19600) loss: 1.061859\n",
      "(Iteration 18601 / 19600) loss: 1.134626\n",
      "(Epoch 19 / 20) train acc: 0.744000; val_acc: 0.599000\n",
      "(Iteration 18621 / 19600) loss: 1.242341\n",
      "(Iteration 18641 / 19600) loss: 1.175588\n",
      "(Iteration 18661 / 19600) loss: 0.997914\n",
      "(Iteration 18681 / 19600) loss: 0.946317\n",
      "(Iteration 18701 / 19600) loss: 1.042635\n",
      "(Iteration 18721 / 19600) loss: 1.200895\n",
      "(Iteration 18741 / 19600) loss: 1.111689\n",
      "(Iteration 18761 / 19600) loss: 1.036566\n",
      "(Iteration 18781 / 19600) loss: 0.929611\n",
      "(Iteration 18801 / 19600) loss: 1.082583\n",
      "(Iteration 18821 / 19600) loss: 0.828353\n",
      "(Iteration 18841 / 19600) loss: 1.194502\n",
      "(Iteration 18861 / 19600) loss: 1.028952\n",
      "(Iteration 18881 / 19600) loss: 1.063508\n",
      "(Iteration 18901 / 19600) loss: 1.092083\n",
      "(Iteration 18921 / 19600) loss: 1.086648\n",
      "(Iteration 18941 / 19600) loss: 0.926372\n",
      "(Iteration 18961 / 19600) loss: 1.272888\n",
      "(Iteration 18981 / 19600) loss: 0.880952\n",
      "(Iteration 19001 / 19600) loss: 1.230746\n",
      "(Iteration 19021 / 19600) loss: 1.097886\n",
      "(Iteration 19041 / 19600) loss: 0.984208\n",
      "(Iteration 19061 / 19600) loss: 0.986222\n",
      "(Iteration 19081 / 19600) loss: 1.112949\n",
      "(Iteration 19101 / 19600) loss: 0.802728\n",
      "(Iteration 19121 / 19600) loss: 0.826108\n",
      "(Iteration 19141 / 19600) loss: 0.985516\n",
      "(Iteration 19161 / 19600) loss: 1.139762\n",
      "(Iteration 19181 / 19600) loss: 1.071387\n",
      "(Iteration 19201 / 19600) loss: 0.908552\n",
      "(Iteration 19221 / 19600) loss: 0.940242\n",
      "(Iteration 19241 / 19600) loss: 0.911548\n",
      "(Iteration 19261 / 19600) loss: 0.974870\n",
      "(Iteration 19281 / 19600) loss: 0.675010\n",
      "(Iteration 19301 / 19600) loss: 1.155650\n",
      "(Iteration 19321 / 19600) loss: 1.388249\n",
      "(Iteration 19341 / 19600) loss: 1.020347\n",
      "(Iteration 19361 / 19600) loss: 0.873716\n",
      "(Iteration 19381 / 19600) loss: 1.151185\n",
      "(Iteration 19401 / 19600) loss: 0.944763\n",
      "(Iteration 19421 / 19600) loss: 0.873401\n",
      "(Iteration 19441 / 19600) loss: 1.090051\n",
      "(Iteration 19461 / 19600) loss: 1.054296\n",
      "(Iteration 19481 / 19600) loss: 1.321476\n",
      "(Iteration 19501 / 19600) loss: 1.166887\n",
      "(Iteration 19521 / 19600) loss: 0.860158\n",
      "(Iteration 19541 / 19600) loss: 0.887424\n",
      "(Iteration 19561 / 19600) loss: 1.273210\n",
      "(Iteration 19581 / 19600) loss: 1.284865\n",
      "(Epoch 20 / 20) train acc: 0.759000; val_acc: 0.591000\n"
     ]
    }
   ],
   "source": [
    "model_full = ThreeLayerFancyNet(weight_scale=0.001, hidden_dim=100, reg=0.005)\n",
    "\n",
    "full_solver = Solver(model_full, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "full_solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filters\n",
    "You can visualize the first-layer convolutional filters from the trained network by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_utils import visualize_grid\n",
    "\n",
    "grid = visualize_grid(model.params['theta1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment!\n",
    "Experiment and try to get the best performance that you can on CIFAR-10 using a ConvNet. Here are some ideas to get you started:\n",
    "\n",
    "### Things you should try:\n",
    "- Filter size: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- Number of filters: Above we used 32 filters. Do more or fewer do better?\n",
    "- Network architecture: The network above has two layers of trainable parameters. Can you do better with a deeper network? You can implement alternative architectures in the file `convnet.py`. Some good architectures to try include:\n",
    "    - [conv-relu-pool]xN - conv - relu - [affine]xM - [softmax or SVM]\n",
    "    - [conv-relu-pool]XN - [affine]XM - [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the course-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at least 65% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training, validation, and test set accuracies for your final trained network. In this notebook you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a really good model on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Description\n",
    "If you implement any additional features for extra credit, clearly describe them here with pointers to any code in this or other files if applicable.\n",
    "\n",
    "We implemented a new Non-Linear Function: Exponential Linear Unit.\n",
    "\n",
    "ELU(X) = x for x>0, = exp(x) -1 for x<= 0\n",
    "\n",
    "This can be seen in the following cells, and be found in layers.py and layers utlis.py\n",
    "\n",
    "All following networks use ELUS.\n",
    "\n",
    "We found that ELU conv net memorizes 50 examples faster than the same network with ReLu.\n",
    "\n",
    "We ran two different modesls BestNet and BestNet2 that have their \"geometries\" in the class doc strings.\n",
    "\n",
    "The Last Network we ran get a Val Accuracy of 73.9%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELU (Eponential Linear Unit Check)\n",
    "After preforming some simple research we found a possibly superior method to Leaky ReLu which was ELU and found the implementation very easy.\n",
    "In the following cells we check our implemntation of ELU which can be found in layer_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_elu_pool\n",
      "dx error:  1.5278856439353933e-07\n",
      "dtheta error:  4.4277686235391647e-10\n",
      "dtheta0 error:  1.001467179037312e-11\n"
     ]
    }
   ],
   "source": [
    "from layer_utils import conv_elu_pool_forward, conv_elu_pool_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 16, 16)\n",
    "theta = np.random.randn(3, 3, 3, 3)\n",
    "theta0 = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "out, cache = conv_elu_pool_forward(x, theta, theta0, conv_param, pool_param)\n",
    "dx, dtheta, dtheta0 = conv_elu_pool_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_elu_pool_forward(x, theta, theta0, conv_param, pool_param)[0], x, dout)\n",
    "dtheta_num = eval_numerical_gradient_array(lambda w: conv_elu_pool_forward(x, theta, theta0, conv_param, pool_param)[0], theta, dout)\n",
    "dtheta0_num = eval_numerical_gradient_array(lambda b: conv_elu_pool_forward(x, theta, theta0, conv_param, pool_param)[0], theta0, dout)\n",
    "print 'Testing conv_elu_pool'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dtheta error: ', rel_error(dtheta_num, dtheta)\n",
    "print 'dtheta0 error: ', rel_error(dtheta0_num, dtheta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if our CONV ELU 3 Layer neural network will learn better than the CONV RELU 3 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvEluNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting ELU Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compared to the same Convolution Net with ReLu which can be seen in a previous cell. This network learns faster significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Custom Convolutional Net to Reach 65% acc\n",
    "Code can be found in cnn.py under BestNet, BestNet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = BestNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the Net Learsn and nearly memorizes 1000 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 31, 31)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BestNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a4bc5748494d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBestNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m solver = Solver(model, small_data,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BestNet' is not defined"
     ]
    }
   ],
   "source": [
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = BestNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model without regulizaration preforms above 65% but still is overfitted with the full data set. Running it again with a slight regularization of .01 improves over fitting but not perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BestNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BestNet(weight_scale=1e-2, reg = .01)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = BestNet2(weight_scale=1e-2, reg = .01)\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solver = Solver(model3, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = BestNet2(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model3, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = BestNet2(weight_scale=1e-2, reg = 0.02)\n",
    "\n",
    "solver = Solver(model4, data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
